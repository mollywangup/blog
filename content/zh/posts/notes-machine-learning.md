---
title: "å­¦ä¹ ç¬”è®°ï¼šå´æ©è¾¾æœºå™¨å­¦ä¹ "
date: 2023-08-04T08:09:47Z
draft: false
description: ç›‘ç£å­¦ä¹ åŒ…æ‹¬çº¿æ€§å›å½’ï¼Œé€»è¾‘å›å½’ï¼ŒSVMï¼Œæœ´ç´ è´å¶æ–¯ï¼Œå†³ç­–æ ‘ï¼Œéšæœºæ£®æ—ï¼ŒXGBoostï¼›æ— ç›‘ç£å­¦ä¹ åŒ…æ‹¬ K-meansï¼ŒPCA ç­‰ã€‚é™„å¸¦å¤ä¹ ç›¸å…³æ•°å­¦åŸºç¡€ã€‚
hideToc: false
enableToc: true
enableTocContent: false
tocPosition: inner
tags:
- Machine Learning
- sklearn
categories:
- Notes
libraries:
- mathjax
---

æœ¬ç¬”è®°åŸºäºä»¥ä¸‹å­¦ä¹ èµ„æ–™ï¼ˆä¾§é‡å®é™…åº”ç”¨ï¼‰ï¼š
> å…¥é—¨æœºå™¨å­¦ä¹ ï¼š[(å¼ºæ¨|åŒå­—)2022å´æ©è¾¾æœºå™¨å­¦ä¹ Deeplearning.aiè¯¾ç¨‹](https://www.bilibili.com/video/BV1Pa411X76s/)
> Python ä»£ç åº“ï¼š[scikit-learn å®˜ç½‘](https://scikit-learn.org/stable/index.html)
> å¤ä¹ çº¿æ€§ä»£æ•°ï¼š3Blue1Brown çš„ [çº¿æ€§ä»£æ•°çš„æœ¬è´¨ - ç³»åˆ—åˆé›†](https://www.bilibili.com/video/BV1ys411472E/)

## ç»Ÿä¸€å£å¾„

### æœ¯è¯­

- ç‰¹å¾ï¼ˆ`feature`ï¼‰ï¼šæŒ‡è¾“å…¥å˜é‡ï¼›
- æ ‡ç­¾ï¼ˆ`label`ï¼‰ï¼šæŒ‡è¾“å‡ºå˜é‡ï¼ŒçœŸå®å€¼ï¼ˆ`target` æˆ– `ground truth`ï¼‰ï¼Œé¢„æµ‹å€¼ï¼ˆ`prediction`ï¼‰ï¼›
- è®­ç»ƒé›†ï¼ˆ`training set`ï¼‰ï¼šæŒ‡ç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†ï¼›
- æµ‹è¯•é›†ï¼ˆ`test set`ï¼‰ï¼šæŒ‡ç”¨äºéªŒè¯æ¨¡å‹çš„æ•°æ®é›†ï¼›
- è®­ç»ƒç¤ºä¾‹ï¼ˆ`training example`ï¼‰ï¼šæŒ‡è®­ç»ƒé›†ä¸­çš„ä¸€ç»„æ•°æ®ï¼›
- æ¨¡å‹ï¼ˆ`model`ï¼‰ï¼šæŒ‡æ‹Ÿåˆå‡½æ•°ï¼›
- æ¨¡å‹å‚æ•°ï¼ˆ`parameter`ï¼‰ï¼šè°ƒæ•´æ¨¡å‹çš„æœ¬è´¨æ˜¯è°ƒæ•´æ¨¡å‹å‚æ•°ï¼›
- [æŸå¤±å‡½æ•°ï¼ˆLoss functionï¼‰](#LossFunction)ï¼šè¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ç¨‹åº¦ï¼Œ"å•ä¸ªæŸå¤±"ï¼›
- æˆæœ¬å‡½æ•°ï¼ˆ`Cost function`ï¼‰ï¼šç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œ"æ€»å¹³å‡æŸå¤±"ï¼›
- ç‰¹å¾å·¥ç¨‹ï¼ˆ`feature engineering`ï¼‰ï¼šå¯¹ç‰¹å¾è¿›è¡Œé€‰æ‹©ã€æå–å’Œè½¬æ¢ç­‰æ“ä½œï¼Œç”¨äºæé«˜æ¨¡å‹æ€§èƒ½ï¼›

### ç¬¦å·<a id="ç¬¦å·"></a>

çº¦å®šå¦‚ä¸‹ï¼š
1. `m` ä¸ªè®­ç»ƒç¤ºä¾‹ï¼Œ`n` ä¸ªç‰¹å¾ï¼›
2. å‘é‡æ˜¯ä¸€ç»´æ•°ç»„ï¼Œä½¿ç”¨å°å†™å­—æ¯è¡¨ç¤ºï¼Œä¸”`é»˜è®¤åˆ—å‘é‡`ï¼›çŸ©é˜µæ˜¯äºŒç»´æ•°ç»„ï¼Œä½¿ç”¨å¤§å†™å­—æ¯è¡¨ç¤ºï¼›

<br>å…·ä½“ç¬¦å·ï¼š
- $x \in \mathbb{R}^n$ è¡¨ç¤ºè¾“å…¥å˜é‡ï¼Œ$w \in \mathbb{R}^n$ è¡¨ç¤ºå›å½’ç³»æ•°ï¼›
- $X \in \mathbb{R}^{m \times n}$ è¡¨ç¤ºè®­ç»ƒç¤ºä¾‹ç»„æˆçš„çŸ©é˜µï¼Œ$y,\hat{y} \in \mathbb{R}^m$ åˆ†åˆ«è¡¨ç¤ºçœŸå®å€¼å’Œé¢„æµ‹å€¼ã€‚
  - $x^{(i)} \in \mathbb{R}^n$ è¡¨ç¤ºç¬¬ $i$ ä¸ªè®­ç»ƒç¤ºä¾‹ï¼›ï¼ˆç¬¬ $i$ è¡Œï¼Œå…¶ä¸­ $i \in [1,m]$ï¼‰
  - $x_j \in \mathbb{R}^m$ è¡¨ç¤ºç¬¬ $j$ ä¸ªç‰¹å¾ï¼›ï¼ˆç¬¬ $j$ åˆ—ï¼Œå…¶ä¸­ $j \in [1,n]$ï¼‰
  - $x_j^{(i)} \in \mathbb{R}$ è¡¨ç¤ºç¬¬ $i$ ä¸ªè®­ç»ƒç¤ºä¾‹çš„ç¬¬ $j$ ä¸ªç‰¹å¾ï¼›
  - $y^{(i)},\hat{y}^{(i)} \in \mathbb{R}$ åˆ†åˆ«è¡¨ç¤ºç¬¬ $i$ ä¸ªè®­ç»ƒç¤ºä¾‹çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼ï¼›

$$
x = \begin{bmatrix}x_1 \\\\ x_2 \\\\ \vdots \\\\ x_n \end{bmatrix}
\space
w = \begin{bmatrix}w_1 \\\\ w_2 \\\\ \vdots \\\\ w_n \end{bmatrix}
\space
y = \begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \vdots \\\\ y^{(m)} \end{bmatrix}
\space
\hat{y} = \begin{bmatrix}\hat{y}^{(1)} \\\\ \hat{y}^{(2)} \\\\ \vdots \\\\ \hat{y}^{(m)} \end{bmatrix}
\space
$$

$$
X =
\begin{bmatrix}
  x_1^{(1)} & x_2^{(1)} & \dots & x_n^{(1)} \\\\ 
  x_1^{(2)} & x_2^{(2)} & \dots & x_n^{(2)} \\\\ 
  \vdots & \vdots & \ddots & \vdots  \\\\ 
  x_1^{(m)} & x_2^{(m)} & \dots & x_n^{(m)} 
\end{bmatrix}
\space
x^{(i)} = \begin{bmatrix}x_1^{(i)} \\\\ x_2^{(i)} \\\\ \vdots \\\\ x_n^{(i)} \end{bmatrix}
\space
x_j = \begin{bmatrix}x_j^{(1)} \\\\ x_j^{(2)} \\\\ \vdots \\\\ x_j^{(m)} \end{bmatrix}
$$

<!-- $$
(X|y) = \left [
\begin{array}{cccc|c}
  x_1^{(1)} & x_2^{(1)} & \dots & x_n^{(1)} & y^{(1)} \\\\ 
  x_1^{(2)} & x_2^{(2)} & \dots & x_n^{(2)} & y^{(2)} \\\\ 
  \vdots & \vdots & \ddots & \vdots & \vdots \\\\ 
  x_1^{(m)} & x_2^{(m)} & \dots & x_n^{(m)} & y^{(m)} 
\end{array}
\right ]
$$ -->

## ç›‘ç£å­¦ä¹ <a id="SupervisedLearning"></a>

{{< alert theme="info" >}}
æœ‰æ ‡ç­¾çš„æ˜¯ç›‘ç£å­¦ä¹ ã€‚é¢„æµ‹è¿ç»­å€¼çš„æ˜¯`å›å½’`ä»»åŠ¡ï¼Œé¢„æµ‹ç¦»æ•£å€¼çš„æ˜¯`åˆ†ç±»`ä»»åŠ¡ã€‚
{{< /alert >}}

ç»™å®š`åŒ…å«æ ‡ç­¾`çš„è®­ç»ƒé›† $(X,y)$ï¼Œé€šè¿‡ç®—æ³•æ„å»ºä¸€ä¸ªé¢„ä¼°å™¨ï¼Œå­¦ä¹ å¦‚ä½•ä» $x$ é¢„æµ‹ $\hat{y}$ï¼Œå³ï¼š$$ (X,y) \to f(x) \space\text{or}\space p(y|x) \to \hat{y} $$

<!-- è¯´æ˜ï¼šä»¥ä¸‹çº¦å®š**åˆ¤åˆ«å¼æ¨¡å‹**ä½¿ç”¨ $f(x)$ï¼Œ**ç”Ÿæˆå¼æ¨¡å‹**ä½¿ç”¨ $p(y|x)$ã€‚ -->

<!-- ç›‘ç£å­¦ä¹ ä»»åŠ¡åˆ†ä¸º`å›å½’ï¼ˆRegressionï¼‰`å’Œ`åˆ†ç±»ï¼ˆClassificationï¼‰`ï¼Œå‰è€…é¢„æµ‹**è¿ç»­å€¼**ï¼Œåè€…é¢„æµ‹**ç¦»æ•£å€¼**ã€‚ -->
<!-- - `å›å½’ï¼ˆRegressionï¼‰`ï¼šå¯ç”¨äºè¶‹åŠ¿é¢„æµ‹ã€ä»·æ ¼é¢„æµ‹ã€æµé‡é¢„æµ‹ç­‰ï¼› -->
<!-- - `åˆ†ç±»ï¼ˆClassificationï¼‰`ï¼šå¯ç”¨äºæ„å»ºç”¨æˆ·ç”»åƒã€ç”¨æˆ·è¡Œä¸ºé¢„æµ‹ã€å›¾åƒè¯†åˆ«åˆ†ç±»ç­‰ï¼› -->

<!-- ç›®æ ‡ï¼šæ¨¡å‹åº”å°½å¯èƒ½æ»¡è¶³ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ç¨‹åº¦ï¼Œä½†åˆä¸èƒ½è¿‡æ‹Ÿåˆï¼ˆæ³›åŒ–èƒ½åŠ›ï¼‰ï¼› -->

<!-- æ€è·¯ï¼šå…ˆé€‰æ‹©ä¸€ä¸ªè®­ç»ƒæ¨¡å‹ï¼Œé‚£æ¨¡å‹å‚æ•°å¦‚ä½•ç¡®å®šå‘¢ï¼Ÿ -->
<!-- æ‹†è§£ç›®æ ‡ï¼š
Step1ï¼šé€‰æ‹©è®­ç»ƒæ¨¡å‹ï¼šå«æ¨¡å‹å‚æ•°ï¼›
Step2ï¼šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼šé€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä»¥è¡¡é‡æ¨¡å‹çš„é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ç¨‹åº¦ï¼›ç¡®å®šæŸå¤±å‡½æ•°ï¼šå°†æ¨¡å‹ä»£å…¥æŸå¤±å‡½æ•°å¾—åˆ°æˆæœ¬å‡½æ•°ï¼Œä»¥é‡åŒ–æ¨¡å‹æ€§èƒ½ï¼›
Step3ï¼šæ±‚è§£ç›®æ ‡ï¼šæ±‚æˆæœ¬å‡½æ•°çš„æå°å€¼è§£ã€‚æ±‚æå°å€¼é—®é¢˜å¸¸ç”¨åˆ°[æ¢¯åº¦ä¸‹é™ç®—æ³•](#GD)ã€‚ -->

### çº¿æ€§å›å½’<a id="LinearRegression"></a>

çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ï¼Œè§£å†³çº¿æ€§çš„**å›å½’**é—®é¢˜ã€‚
<!-- å‰æå‡è®¾æ˜¯é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„è¯¯å·®ï¼ˆerrorï¼‰æœä»æ­£æ€åˆ†å¸ƒã€‚ -->

#### åŸç†

##### æ¨¡å‹

$n$ å…ƒçº¿æ€§å›å½’çš„æ¨¡å‹ $f(x): \mathbb{R}^n \to \mathbb{R}$ å¦‚ä¸‹ï¼š

$$ 
f_{w,b}(x) = w \cdot x + b = 
\begin{bmatrix}w_1 \\\\ w_2 \\\\ \vdots \\\\ w_n \end{bmatrix} 
\cdot 
\begin{bmatrix} x_1 \\\\ x_2 \\\\ \vdots \\\\ x_n \end{bmatrix} + b =
\sum_{j=1}^{n}w_jx_j + b 
$$

å…¶ä¸­ï¼Œæ¨¡å‹å‚æ•°ï¼š
$w \in \mathbb{R}^n$ï¼šå›å½’ç³»æ•°ï¼Œåˆ†åˆ«å¯¹åº” n ä¸ªç‰¹å¾çš„æƒé‡ï¼ˆweightsï¼‰æˆ–ç³»æ•°ï¼ˆcoefficientsï¼‰ï¼›
$b \in \mathbb{R}$ï¼šåå·®ï¼ˆbiasï¼‰æˆ–æˆªè·ï¼ˆinterceptï¼‰ï¼›

##### æˆæœ¬å‡½æ•°

ä½¿ç”¨[æœ€å°äºŒä¹˜](#LeastSquaresLoss)æŸå¤±ï¼š

$$
\begin{split}
L(w,b) &= \frac{1}{2} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\\\
\\\\&= \frac{1}{2} (w \cdot x^{(i)} + b - y^{(i)})^2 
\end{split}
$$

åŸºäºæœ€å°äºŒä¹˜æŸå¤±ï¼Œå¸¸è§çš„ä¸‰ç§æˆæœ¬å‡½æ•°ï¼š

$$ J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \tag{OLS} $$

<!-- $$ J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2 + \lambda \lVert w \rVert_1 \tag{Lasso} $$ -->

$$ J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} \lvert w_j \rvert \tag{Lasso} $$

<!-- $$ J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \lVert w \rVert_2^2 \tag{Ridge} $$ -->

$$ J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} \lvert w_j \rvert^2 \tag{Ridge} $$

è¯´æ˜ï¼š
1. ä½¿ç”¨ $\frac{1}{2m}$ å–å‡å€¼ï¼Œä»…æ˜¯ä¸ºäº†åœ¨æ±‚ï¼ˆåï¼‰å¯¼æ•°æ—¶æ¶ˆå»å¸¸æ•° $2$ï¼Œä¸å½±å“ç»“æœï¼›
2. `OLS`ï¼šæ™®é€šæœ€å°äºŒä¹˜å›å½’ï¼›
3. `Lasso`ï¼šç”¨äº**ç‰¹å¾é€‰æ‹©**ã€‚æ˜¯åœ¨ OLS çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ äº† $w$ çš„ [L1 èŒƒæ•°](#VectorNorms) ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼›
4. `Ridge`ï¼šç”¨äº[é˜²æ­¢è¿‡æ‹Ÿåˆ](#Underfitting-and-Overfitting)ã€‚æ˜¯åœ¨ OLS çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ äº† $w$ çš„ [L2 èŒƒæ•°](#VectorNorms) çš„å¹³æ–¹ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼›
5. $\lambda$ï¼šè¶…å‚æ•°ï¼Œéè´Ÿæ ‡é‡ï¼Œä¸ºäº†æ§åˆ¶æƒ©ç½šé¡¹çš„å¤§å°ã€‚

{{< expand "çŸ©é˜µä¹˜å‘é‡å†™æ³• ">}}

$$
J(w,b) = \frac{1}{2m} \lVert X_{new} \cdot w_{new} - y \rVert_2^2
$$

å…¶ä¸­ï¼š
$$
(X_{new}|y) = \left [
\begin{array}{ccccc|c}
  1 & x_1^{(1)} & x_2^{(1)} & \dots & x_n^{(1)} & y^{(1)} \\\\ 
  1 & x_1^{(2)} & x_2^{(2)} & \dots & x_n^{(2)} & y^{(2)} \\\\ 
  \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\\\ 
  1 & x_1^{(m)} & x_2^{(m)} & \dots & x_n^{(m)} & y^{(m)} 
\end{array}
\right ]
\space
w_{new} = \begin{bmatrix}b \\\\ w_1 \\\\ w_2 \\\\ \vdots \\\\ w_n \end{bmatrix}
$$

{{< /expand >}}

##### ç›®æ ‡

æ±‚è§£ä¸€ç»„æ¨¡å‹å‚æ•° $(w,b)$ ä½¿å¾—æˆæœ¬å‡½æ•° $J$ æœ€å°åŒ–ã€‚

$$ \min_{w,b} J(w,b) $$

#### ä»£ç 

##### ä¸€å…ƒçº¿æ€§å›å½’

ä»¥ä¸‹ç¤ºä¾‹æ¥æºäº sklearn çš„ç³–å°¿ç—…æ•°æ®é›†ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# åŠ è½½æ•°æ®é›†ï¼šä»…å–å…¶ä¸­ä¸€ä¸ªç‰¹å¾ï¼Œå¹¶æ‹†åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†ï¼ˆ7/3ï¼‰
features, target = load_diabetes(return_X_y=True)
feature = features[:, np.newaxis, 2]
X_train, X_test, y_train, y_test = train_test_split(feature, target, test_size=0.3, random_state=8)
print('ç‰¹å¾æ•°é‡ï¼š{} ä¸ªï¼ˆåŸå§‹æ•°æ®é›†å…± {} ä¸ªç‰¹å¾ï¼‰\næ€»æ ·æœ¬é‡ï¼šå…± {} ç»„ï¼Œå…¶ä¸­è®­ç»ƒé›† {} ç»„ï¼Œæµ‹è¯•é›† {} ç»„'.format(feature.shape[1], features.shape[1], target.shape[0], X_train.shape[0], X_test.shape[0]))

# åˆ›å»ºçº¿æ€§å›å½’æ¨¡å‹å¹¶æ‹Ÿåˆæ•°æ®
model = LinearRegression()
model.fit(X_train, y_train)

# è·å–æ¨¡å‹å‚æ•°
w = model.coef_
b = model.intercept_
print('æ¨¡å‹å‚æ•°ï¼šw={}, b={}'.format(w, b))

# è¡¡é‡æ¨¡å‹æ€§èƒ½ï¼šR2 å’Œ MSE
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
# R2ï¼ˆå†³å®šç³»æ•°ï¼Œ1æœ€ä½³ï¼‰ï¼Œè®¡ç®—ç­‰åŒäº r2_score(y_true, y_pred)
r2_train = model.score(X_train, y_train)
r2_test = model.score(X_test, y_test)
# MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
print('æ¨¡å‹æ€§èƒ½ï¼š\n  è®­ç»ƒé›†ï¼šR2={:.3f}, MSE={:.3f}\n  æµ‹è¯•é›†ï¼šR2={:.3f}, MSE={:.3f}'.format(r2_train, mse_train, r2_test, mse_test))

# ç»˜å›¾
plt.title('LinearRegression (One variable)')
plt.scatter(X_train, y_train, color='red', marker='X')
plt.plot(X_test, y_pred, linewidth=3)
plt.legend(['training points', 'model: $y={:.2f}x+{:.2f}$'.format(w[0], b)])
plt.savefig('LinearRegression_diabetes.svg')
```
<img src='https://user-images.githubusercontent.com/46241961/273402064-fdd2a737-a691-45bc-8c17-6f921e02d487.svg' alt='ä¸€å…ƒçº¿æ€§å›å½’-ç³–å°¿ç—…æ•°æ®é›†' width=80%>

##### å¤šå…ƒçº¿æ€§å›å½’

ä»¥ä¸‹ç¤ºä¾‹æ¥æºäº sklearn çš„ç³–å°¿ç—…æ•°æ®é›†ï¼Œé€‰å–äº†æ‰€æœ‰çš„ç‰¹å¾ï¼Œå¹¶å¯¹æ¯”äº†æ™®é€šæœ€å°äºŒä¹˜/Lasso/Ridge ä¸‰ç§å›å½’æ¨¡å‹çš„æ€§èƒ½ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# åŠ è½½æ•°æ®é›†ï¼šå–æ‰€æœ‰ç‰¹å¾ï¼Œå¹¶æ‹†åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†ï¼ˆ7/3ï¼‰
features, target = load_diabetes(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=8)
print('ç‰¹å¾æ•°é‡ï¼š{} ä¸ª\næ€»æ ·æœ¬é‡ï¼šå…± {} ç»„ï¼Œå…¶ä¸­è®­ç»ƒé›† {} ç»„ï¼Œæµ‹è¯•é›† {} ç»„'.format(features.shape[1], target.shape[0], X_train.shape[0], X_test.shape[0]))

def _models(alpha=1):
    lr = LinearRegression().fit(X_train, y_train) # ç¬¬ä¸€ç§ï¼šæ™®é€šæœ€å°äºŒä¹˜å›å½’
    lasso = Lasso(alpha=alpha).fit(X_train, y_train) # ç¬¬äºŒç§ï¼šLasso/L1/å¥—ç´¢å›å½’
    ridge = Ridge(alpha=alpha).fit(X_train, y_train) # ç¬¬ä¸‰ç§ï¼šRidge/L2/å²­å›å½’
    return lr, lasso, ridge

# å¯¹æ¯”å››ç»„ alpha å–å€¼
alphas_list = [0.05, 0.1, 0.5, 1]

for i in range(len(alphas_list)):
    alpha = alphas_list[i]
    print('\n======== alpha={} ========'.format(alpha))
    
    # å¯¹æ¯”ä¸‰ç§çº¿æ€§æ¨¡å‹
    models = _models(alpha=alpha)
    for model in models:    
        # æ¨¡å‹å‚æ•°
        w = model.coef_
        b = model.intercept_

        # æ¨¡å‹æ€§èƒ½ï¼šR2 å’Œ MSE
        r2_train = model.score(X_train, y_train)
        r2_test = model.score(X_test, y_test)
        mse_train = mean_squared_error(y_train, model.predict(X_train))
        mse_test = mean_squared_error(y_test, model.predict(X_test))
    
        # æ‰“å°
        model_name = model.__class__.__name__
        print('{}ï¼š\n  æ¨¡å‹å‚æ•°ï¼šw={}, b={:.3f}\n  è®­ç»ƒé›†ï¼šR2={:.3f}, MSE={:.3f}\n  æµ‹è¯•é›†ï¼šR2={:.3f}, MSE={:.3f}'.format(model_name, w, b, r2_train, mse_train, r2_test, mse_test))
```

### é€»è¾‘å›å½’<a id="LogisticRegression"></a>

é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰ï¼Œè§£å†³`äºŒåˆ†ç±»`ï¼ˆBinary Classificationï¼‰é—®é¢˜ã€‚

#### åŸç†

##### æ¨¡å‹

ä»¤ $$ z = w \cdot x + b $$ ä½œä¸ºæ–°çš„è¾“å…¥ï¼Œé€šè¿‡ [Sigmoid](https://mollywangup.com/posts/notes-deep-learning/#sigmoid) æ¿€æ´»å‡½æ•°ï¼Œä½¿è¾“å‡ºå€¼åˆ†å¸ƒä»¥ $0.5$ ä¸ºåˆ†ç•Œï¼š 

$$
p(y=1|x;w,b) = g(z) = \frac{1}{1 + e^{-(w \cdot x + b)}}
$$

å½“ $p \geq 0.5$ æ—¶ï¼Œå– $1$ï¼Œå¦åˆ™å– $0$

##### æˆæœ¬å‡½æ•°

ä½¿ç”¨[äº¤å‰ç†µæŸå¤±](#CrossEntropyLoss)ï¼š

$$ L(\hat{y}, y) = -y\ln\hat{y} - (1-y)\ln(1-\hat{y}) $$

å¯¹åº”çš„æˆæœ¬å‡½æ•°ï¼š

$$ J(w,b) = \frac{1}{m} \sum_{i=1}^{m} -y^{(i)} \ln \hat y^{(i)} - (1-y^{(i)}) \ln(1 - \hat y^{(i)}) $$

##### ç›®æ ‡

æ±‚è§£ä¸€ç»„æ¨¡å‹å‚æ•° $(w,b)$ ä½¿å¾—æˆæœ¬å‡½æ•° $J$ æœ€å°åŒ–ã€‚

$$ \min_{w,b} J(w,b) $$

<!-- true: 1, positive class
false: 0, negative class -->

### SVM<a id="SVM"></a>

æ”¯æŒå‘é‡æœºï¼Œè§£å†³**åˆ†ç±»**é—®é¢˜ã€‚

å±äºçº¿æ€§åˆ†ç±»å™¨ã€‚éçº¿æ€§é—®é¢˜ï¼Œå¯é€šè¿‡ kernal SVM è§£å†³ï¼ˆæ˜ å°„åˆ°é«˜ç»´ï¼‰ï¼›

è¶…å¹³é¢ï¼š
- å†³ç­–åˆ†ç•Œé¢ï¼ˆdecision boundaryï¼‰
- è¾¹ç•Œåˆ†ç•Œé¢ï¼ˆmargin boundaryï¼‰

Hard-margin SVM
Soft-margin SVMï¼šåŠ å…¥äº†å®¹é”™ç‡

<!-- ### KNN<a id="K-NearestNeighbors"></a>

KNN (K-Nearest Neighbors)ï¼Œè§£å†³**åˆ†ç±»+å›å½’**é—®é¢˜ã€‚`K ä¸ªé‚»å±…çš„æ„æ€`ã€‚

ç»™å®šè®­ç»ƒé›† $(X,y)$ï¼ŒKNN è¦å®ç°çš„æ˜¯å°† -->

### æœ´ç´ è´å¶æ–¯<a id="NaiveBayes"></a>

Naive Bayesï¼Œè§£å†³**åˆ†ç±»**é—®é¢˜ã€‚

### å†³ç­–æ ‘<a id="DecisionTree"></a>

Decision treeï¼Œè§£å†³**åˆ†ç±»**é—®é¢˜ã€‚

- æ ¹èŠ‚ç‚¹ï¼šæ— å…¥å¤šå‡º
- å†…éƒ¨èŠ‚ç‚¹ï¼šä¸€å…¥å¤šå‡º
- å¶å­ç»“ç‚¹ï¼šä¸€å…¥æ— å‡º

ç†µ

åŸºå°¼ç³»æ•°

### éšæœºæ£®æ—<a id="RandomForest"></a>

æœ‰æ”¾å›éšæœºæŠ½å­é›†ã€‚

Random forestï¼Œè§£å†³**åˆ†ç±»**é—®é¢˜ã€‚

å›å½’é—®é¢˜ï¼šæ±‚å‡å€¼
åˆ†åˆ—é—®é¢˜ï¼šæ±‚ä¼—æ•°

### XGBoost<a id="XGBoost"></a>


## æ— ç›‘ç£å­¦ä¹ <a id="UnsupervisedLearning"></a>

{{< alert theme="info" >}}
æ— æ ‡ç­¾çš„æ˜¯æ— ç›‘ç£å­¦ä¹ ã€‚
{{< /alert >}}

ç»™å®š`ä¸åŒ…å«æ ‡ç­¾`çš„è®­ç»ƒé›† $X$ï¼Œé€šè¿‡ç®—æ³•æ„å»ºä¸€ä¸ªé¢„ä¼°å™¨ï¼Œæ­ç¤ºæ•°æ®çš„å†…åœ¨åˆ†å¸ƒç‰¹æ€§åŠè§„å¾‹ï¼Œå³ï¼š$$ X \to f(x) \to \hat{y} $$

æ— ç›‘ç£å­¦ä¹ ä»»åŠ¡åˆ†ä¸º`èšç±»ï¼ˆClusteringï¼‰`å’Œ`é™ç»´ï¼ˆDimensionality reductionï¼‰`ã€‚

<br><img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png' alt='èšç±»æ–¹æ³•å¯¹æ¯”ï¼ˆå›¾æº scikit-learnï¼‰' width=80%>

### K-means

è§£å†³**èšç±»**é—®é¢˜ã€‚`K ä¸ªç±»åˆ«çš„æ„æ€`ã€‚

#### åŸç†

ç»™å®šè®­ç»ƒé›† $X \in \mathbb{R}^{m \times n}$ï¼ŒK-means è¦å®ç°çš„æ˜¯å°† $m$ ä¸ªç‚¹ï¼ˆè®­ç»ƒç¤ºä¾‹ï¼‰èšç±»ä¸º $k$ ä¸ªç°‡ï¼ˆClusterï¼‰ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š

æ­¥éª¤ä¸€ï¼šéšæœºåˆå§‹åŒ– $k$ ä¸ªç°‡ä¸­å¿ƒï¼Œè®°ä½œ $\mu_j \in \mathbb{R}^n$ï¼›
æ­¥éª¤äºŒï¼šä¸ºæ¯ä¸ªç‚¹ $x^{(i)}$ åˆ†é…è·ç¦»æœ€è¿‘çš„ç°‡ï¼Œè®°ä½œ $c^{(i)}$ï¼š$$ c^{(i)} = \displaystyle\min_{j} \lVert x^{(i)} - \mu_j\rVert_2^2 $$
æ­¥éª¤ä¸‰ï¼šä¸ºæ¯ä¸ªç°‡é‡æ–°è®¡ç®—ç°‡ä¸­å¿ƒ $\mu_{j}$ï¼Œæ–¹æ³•æ˜¯è¯¥ç°‡ä¸­æ‰€æœ‰ç‚¹çš„å‡å€¼ï¼›

é‡å¤ä»¥ä¸Šæ­¥éª¤äºŒå’Œæ­¥éª¤ä¸‰ï¼Œç›´è‡³ $k$ ä¸ªç°‡ä¸­å¿ƒä¸å†å‘ç”Ÿå˜åŒ–ï¼ˆå³æ”¶æ•›ï¼‰ã€‚

æˆæœ¬å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
J(c^{(1)}, \cdots, c^{(m)}, \mu_1, \cdots, \mu_k) = \frac{1}{m} \sum_{i=1}^{m} \lVert x^{(i)} - \mu_{c^{(i)}}\rVert_2^2
$$

å…¶ä¸­ï¼š$\mu_{c^{(i)}}$ è¡¨ç¤º $x^{(i)}$ æ‰€å±çš„ç°‡ä¸­å¿ƒï¼›

ä¼˜åŒ–åˆå§‹çš„ k ä¸ªç°‡ä¸­å¿ƒé€‰æ‹©ï¼š

1. ä» $X$ ä¸­é€‰æ‹©ï¼›
2. 

#### ä»£ç 

```python
import time
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import pairwise_distances_argmin

# æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
np.random.seed(0)

batch_size = 45
centers = np.array([[1, 1], [-1, -1], [1, -1]])
n_clusters = centers.shape[0]
X, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=[0.3, 0.7, 1])

# ä½¿ç”¨ K-means èšç±»
k_means = KMeans(init='k-means++', n_clusters=3, n_init=10)
t0 = time.time()
k_means.fit(X)
t_batch = time.time() - t0

# æ ¡éªŒ
k_means_cluster_centers = k_means.cluster_centers_
k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)

# ç»˜å›¾
fig = plt.figure(figsize=(8, 3))
fig.subplots_adjust(left=0.01, right=0.98, bottom=0.05, top=0.9)
colors = ["#4EACC5", "#FF9C34", "#4E9A06"]

ax = fig.add_subplot(1, 3, 1)
for k, col in zip(range(n_clusters), colors):
    my_members = k_means_labels == k
    cluster_center = k_means_cluster_centers[k]
    ax.plot(X[my_members, 0], X[my_members, 1], "w", markerfacecolor=col, marker=".")
    ax.plot(
        cluster_center[0],
        cluster_center[1],
        "o",
        markerfacecolor=col,
        markeredgecolor="k",
        markersize=6,
    )
ax.set_title("KMeans")
ax.set_xticks(())
ax.set_yticks(())
# plt.text(-3.5, 1.8, "train time: %.2fs\ninertia: %f" % (t_batch, k_means.inertia_))
```

### DBSCAN

è§£å†³**èšç±»**é—®é¢˜ã€‚

- DBSCANï¼ˆå¯†åº¦èšç±»ï¼‰ï¼šå°† n ä¸ªç‚¹åˆ†ä¸ºä¸‰ç±»ï¼Œç„¶ååˆ é™¤å™ªéŸ³ç‚¹ï¼›ï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼‰
  - æ ¸å¿ƒç‚¹ï¼šåœ¨åŠå¾„ epsï¼ˆä¸¤ä¸ªæ ·æœ¬è¢«çœ‹åšé‚»åŸŸçš„æœ€å¤§ä¸¾ä¾‹ï¼‰ å†…çš„ç‚¹çš„ä¸ªæ•°è¶…è¿‡ min_samplesï¼ˆç°‡çš„æ ·æœ¬æ•°ï¼‰ï¼›
  - è¾¹ç•Œç‚¹ï¼šåœ¨åŠå¾„ eps å†…çš„ç‚¹çš„ä¸ªæ•°ä¸è¶…è¿‡ min_samplesï¼Œä½†è½åœ¨æ ¸å¿ƒç‚¹çš„é‚»åŸŸå†…ï¼›
  - å™ªéŸ³ç‚¹ï¼šæ—¢ä¸æ˜¯æ ¸å¿ƒç‚¹ï¼Œä¹Ÿä¸æ˜¯è¾¹ç•Œç‚¹ï¼›

### PCA<a id="PrincipalComponentAnalysis"></a>

ä¸»æˆåˆ†åˆ†æï¼ˆPrincipal Component Analysis, PCAï¼‰ï¼Œè§£å†³**é™ç»´**é—®é¢˜ã€‚

ç”¨æœ€å°‘çš„ç‰¹å¾å°½å¯èƒ½è§£é‡Šæ‰€æœ‰çš„æ–¹å·®ï¼ˆè¶Šç¦»æ•£æ–¹å·®è¶Šå¤§ï¼‰ã€‚

ç”¨é€”ï¼šå¯è§†åŒ–ï¼Œ

<!-- ## å¼ºåŒ–å­¦ä¹ 

ï¼ˆReinforcement Learningï¼‰ï¼šæœ‰å»¶è¿Ÿå’Œç¨€ç–çš„åé¦ˆæ ‡ç­¾ï¼› -->

## ç‰¹å¾å·¥ç¨‹

<!-- æŒ–å‘ï¼š
ç¼ºå¤±å€¼å¤„ç†
å¼‚å¸¸å€¼å¤„ç† -->

### å¤šé¡¹å¼ç‰¹å¾<a id="PolynomialFeatures"></a>

{{< alert theme="info" >}}
é€šè¿‡æ·»åŠ `ç‰¹å¾çš„å¤šé¡¹å¼`æ¥æé«˜æ¨¡å‹å¤æ‚åº¦ï¼Œå°†å…¶è§†ä½œæ–°ç‰¹å¾åˆ™å½’æ¥ä»æ˜¯[çº¿æ€§å›å½’](#LinearRegression)é—®é¢˜ã€‚
{{< /alert >}}

ä¾‹å­ï¼šä»¥ä¸‹å¼ $(1)(2)(3)$ ä¾æ¬¡å¯¹åº”ä¸€å…ƒäºŒæ¬¡å¤šé¡¹å¼ã€ä¸€å…ƒä¸‰æ¬¡å¤šé¡¹å¼ã€äºŒå…ƒäºŒæ¬¡å¤šé¡¹å¼æ¨¡å‹ï¼š

$$ f_{w,b}(x) = w_1x + w_2x^2 + b \tag{1} $$

$$ f_{w,b}(x) = w_1x + w_2x^2 + w_3x^3 + b \tag{2} $$

$$ f_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_1x_2 + w_4x_1^2 + w_5x_2^2 + b \tag{3} $$

ä»¥å¼ $(1)$ çš„æ¨¡å‹ä¸ºä¾‹ï¼Œå°†éçº¿æ€§çš„ $f(x) \to y$ é—®é¢˜ï¼Œè½¬åŒ–ä¸ºçº¿æ€§çš„ $f(x,x^2) \to y$ é—®é¢˜ï¼Œå³`å°†éä¸€æ¬¡é¡¹çš„ $x^2$ è§†ä½œæ–°ç‰¹å¾`ï¼Œå³å¯æŒ‰ç…§çº¿æ€§å›å½’æ¨¡å‹è®­ç»ƒã€‚

{{< expand "ä»£ç ï¼šä»¥ä¸€å…ƒç‰¹å¾ä¸ºä¾‹ï¼Œå¯¹æ¯”ä¸åŒ degree çš„æ¨¡å‹è´¨é‡" >}}

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

rng = np.random.RandomState(0)

# æ•°æ®é›†
x = np.linspace(-3, 7, 10)
y = np.power(x, 3) + np.power(x, 2) + x + 1 + rng.randn(1)
X = x[:, np.newaxis]

# ç»˜åˆ¶è®­ç»ƒé›†
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(X, y, color='red', marker='X', label='training points')

# å¤šé¡¹å¼ç‰¹å¾çš„çº¿æ€§å›å½’æ¨¡å‹
for degree in range(10):
    # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
    poly = PolynomialFeatures(degree)
    X_poly = poly.fit_transform(X)
    
    # åˆ›å»ºçº¿æ€§å›å½’æ¨¡å‹ï¼šX_poly ä¸ y ä¸ºçº¿æ€§å…³ç³»
    model = LinearRegression()
    model.fit(X_poly, y)

    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹
    y_pred = model.predict(X_poly)
    
    # è·å–æ¨¡å‹å‚æ•°å’Œæ€§èƒ½æŒ‡æ ‡
    w = model.coef_
    b = model.intercept_
    r2 = model.score(X_poly, y)
    mse = mean_squared_error(y, y_pred)

    # ç»˜å›¾
    ax.plot(X, y_pred, label='Degree {}: MSE {:.3f}, $R^2$ {:.3f}'.format(degree, round(mse, 3), r2))

# æ·»åŠ å›¾ä¾‹
plt.legend(loc='best', fontsize='small')
plt.savefig('PolynomialFeatures_LinearRegression.svg')
plt.show()
```
{{< /expand >}}

<img src='https://user-images.githubusercontent.com/46241961/278821723-d779c271-25a2-470f-88ee-3c0643ea69e1.svg' alt='PolynomialFeatures_LinearRegression' width='80%'>

### ç‰¹å¾ç¼©æ”¾

ç‰¹å¾ç¼©æ”¾ï¼ˆFeature Scalingï¼‰æ˜¯ä¸€ç§ç”¨äº**æ ‡å‡†åŒ–è‡ªå˜é‡æˆ–ç‰¹å¾èŒƒå›´**çš„æ–¹æ³•ã€‚

èƒŒæ™¯ï¼šä¸åŒç‰¹å¾ä¹‹é—´çš„å–å€¼èŒƒå›´å·®å¼‚è¾ƒå¤§ï¼Œå¯¼è‡´æ¢¯åº¦ä¸‹é™è¿è¡Œä½æ•ˆã€‚ç‰¹å¾ç¼©æ”¾ä½¿å¾—ä¸åŒç‰¹å¾ä¹‹é—´çš„å–å€¼èŒƒå›´å·®å¼‚ï¼Œé™ä½è‡³å¯æ¯”è¾ƒçš„èŒƒå›´ã€‚
  - é™¤ä¸Šé™ï¼Œå¦‚ [200, 1000] -> [0.2, 1]

ç›®æ ‡ï¼šä¸ºäº†ä½¿æ¢¯åº¦ä¸‹é™è¿è¡Œçš„æ›´å¿«ï¼Œæœ€ç»ˆæé«˜æ¨¡å‹è®­ç»ƒæ€§èƒ½ã€‚

ç»éªŒå€¼ï¼š
- å¤ªå¤§æˆ–è€…å¤ªå°éƒ½éœ€è¦ï¼šå¦‚[-0.001, 0.001]ã€[-100, 100]ï¼›
- é€šå¸¸[-3, 3]èŒƒå›´å†…ï¼Œä¸éœ€è¦ï¼›

#### å‡å€¼å½’ä¸€åŒ–

Mean Normalizationï¼Œä¸å‡å€¼çš„å·®å¼‚ / ä¸Šä¸‹é™çš„æ•´ä½“å·®å¼‚ï¼š

$$
x^{\prime} = \frac{x - \mu}{max(x) - min(x)}
$$

#### Z åˆ†æ•°å½’ä¸€åŒ–

Z-score normalizationï¼Œä¸å‡å€¼çš„å·®å¼‚ / æ ‡å‡†å·®ï¼š

$$
x^{\prime} = \frac{x - \mu}{\sigma}
$$

å…¶ä¸­æ ‡å‡†å·®ï¼ˆStandard Deviationï¼‰$\sigma$ è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

$$
\sigma = \sqrt{\frac{\sum {(x - \mu)}^2}{n}}
$$

## æŸå¤±å‡½æ•°<a id='LossFunction'></a>

{{< alert theme="info" >}}
æŸå¤±å‡½æ•°ç”¨äº`è¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ç¨‹åº¦`ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹çš„æ‹Ÿåˆç¨‹åº¦ã€‚
{{< /alert >}}

ç»™å®š $\hat{y},y \in \mathbb{R}$ï¼Œåˆ†åˆ«è¡¨ç¤ºé¢„æµ‹å€¼å’ŒçœŸå®å€¼ï¼Œåˆ™æŸå¤±å‡½æ•°è¡¨ç¤ºä¸ºï¼š$$ L(\hat{y}, y) $$

æˆæœ¬å‡½æ•° $J$ è¡¨ç¤ºä¸ºï¼š

$$
J = \frac{1}{m} \displaystyle \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)
$$

è¯´æ˜ï¼šæˆæœ¬å‡½æ•°æ›´çµæ´»ï¼Œæœ‰æ—¶ä¼šåœ¨æŸå¤±å‡½æ•°çš„åŸºç¡€ä¸Šå†åŠ ä¸Šæ­£åˆ™é¡¹ï¼›

### æœ€å°äºŒä¹˜<a id="LeastSquaresLoss"></a>

$$ L(\hat{y}, y) = \frac{1}{2} (\hat{y} - y)^2 $$

### äº¤å‰ç†µ<a id="CrossEntropyLoss"></a>

æ¨å¯¼è¯¦è§[äº¤å‰ç†µ](#CrossEntropy)

$$ L(\hat{y}, y) = H(y,\hat{y}) = - \sum_x y \ln \hat{y} $$

å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼š$$ L(\hat{y}, y) = -y\ln\hat{y} - (1-y)\ln(1-\hat{y}) $$

## ä¼˜åŒ–ç®—æ³•

### æ¢¯åº¦ä¸‹é™ç®—æ³•<a id="GD"></a>

{{< alert theme="info" >}}
æ ¸å¿ƒåŸç†æ˜¯å¯å¾®å‡½æ•° **`åœ¨æŸç‚¹æ²¿ç€æ¢¯åº¦åæ–¹å‘ï¼Œå‡½æ•°å€¼ä¸‹é™æœ€å¿«ã€‚`**
{{< /alert >}}

æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descent, GDï¼‰æ˜¯ä¸€ç§è¿­ä»£ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºæ±‚è§£ä»»æ„ä¸€ä¸ªå¯å¾®å‡½æ•°çš„`å±€éƒ¨æœ€å°å€¼`ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¸¸ç”¨äº**æœ€å°åŒ–æˆæœ¬å‡½æ•°**ï¼Œå³ï¼š

ç»™å®šæˆæœ¬å‡½æ•° $J(w,b)$ï¼Œæ±‚è§£ä¸€ç»„ $(w,b)$ï¼Œä½¿å¾—
$$ \min_{w,b} J(w,b) $$

#### å®ç°æ–¹æ³•

æ­¥éª¤ä¸€ï¼šé€‰å®šåˆå§‹ç‚¹ $(w_{init},b_{init})$ï¼›
æ­¥éª¤äºŒï¼šä¸ºäº†ä½¿å‡½æ•°å€¼ä¸‹é™ï¼Œéœ€è¦`æ²¿ç€æ¢¯åº¦åæ–¹å‘è¿­ä»£`ï¼Œå³é‡å¤ä»¥ä¸‹æ­¥éª¤ï¼Œç›´è‡³æ”¶æ•›ï¼Œå³å¯å¾—åˆ°å±€éƒ¨æœ€å°å€¼çš„è§£ï¼š

$$
w \leftarrow w - \alpha \frac{\partial J}{\partial w}
$$

$$
b \leftarrow b - \alpha \frac{\partial J}{\partial b}
$$

å³ï¼š

$$
\begin{equation} 
  \begin{pmatrix}
    w_1 \\\\
    w_2 \\\\
    \vdots \\\\
    w_n \\\\
    b
  \end{pmatrix}
    \leftarrow
  \begin{pmatrix}
    w_1 \\\\
    w_2 \\\\
    \vdots \\\\
    w_n \\\\
    b
  \end{pmatrix}
    - \alpha
  \begin{pmatrix}
    \frac{\partial J}{\partial w_1} \\\\
    \frac{\partial J}{\partial w_2} \\\\
    \vdots \\\\
    \frac{\partial J}{\partial w_n} \\\\
    \frac{\partial J}{\partial b} 
  \end{pmatrix}
\end{equation}
$$

å…¶ä¸­ï¼š$\alpha \geq 0$ æŒ‡`å­¦ä¹ ç‡`ï¼ˆLearning rateï¼‰ï¼Œä¹Ÿç§°ä½œæ­¥é•¿ï¼Œå†³å®šäº†è¿­ä»£çš„æ¬¡æ•°ã€‚

<!-- #### é€‰æ‹©å­¦ä¹ ç‡

æ–¹æ³•ï¼šç»™å®šä¸åŒ $\alpha$ è¿è¡Œæ¢¯åº¦ä¸‹é™æ—¶ï¼Œç»˜åˆ¶ $J$ å’Œ è¿­ä»£æ¬¡æ•°çš„å›¾ï¼Œé€šè¿‡è§‚å¯Ÿ $J$ **æ˜¯å¦å•è°ƒé€’å‡ç›´è‡³æ”¶æ•›**æ¥åˆ¤æ–­ $\alpha$ çš„é€‰æ‹©æ˜¯å¦åˆé€‚ï¼›
  - å•è°ƒé€’å¢æˆ–æœ‰å¢æœ‰å‡ï¼š$\alpha$ å¤ªå¤§ï¼Œæ­¥å­è¿ˆå¤§äº†ï¼Œåº”è¯¥é™ä½ $\alpha$ï¼›
  - å•è°ƒé€’å‡ä½†æœªæ”¶æ•›ï¼š$\alpha$ å¤ªå°ï¼Œå­¦ä¹ å¤ªæ…¢ï¼Œåº”è¯¥æå‡ $\alpha$ï¼›

ç»éªŒå€¼å‚è€ƒï¼š[0.001, 0.01, 0.1, 1] æˆ–è€… [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1] -->

#### æ‰¹é‡æ¢¯åº¦ä¸‹é™<a id="BGD"></a>

ï¼ˆBatch Gradient Descent, BGDï¼‰ï¼šä½¿ç”¨è®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ•°æ®

#### éšæœºæ¢¯åº¦ä¸‹é™<a id="SGD"></a>

ï¼ˆstotastic gradient descent, SGDï¼‰ï¼šï¼Ÿï¼Ÿæ ¹æ®æ¯ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œå‚æ•°æ›´æ–°

## æ¨¡å‹è¯„ä¼°

### è¿‡æ‹Ÿåˆé—®é¢˜<a id="Underfitting-and-Overfitting"></a>

å®šä¹‰è¿‡æ‹Ÿåˆï¼šè®­ç»ƒæ–¹å·®å°ï¼Œæµ‹è¯•æ–¹å·®å¤§ã€‚

è§£å†³è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼š
1. æ”¶é›†æ›´å¤šçš„è®­ç»ƒç¤ºä¾‹ï¼›
2. ç‰¹å¾é€‰æ‹©ï¼›
3. æ­£åˆ™åŒ–ï¼›

<img src='https://user-images.githubusercontent.com/46241961/278217087-8b868e06-28d3-4a36-bec8-7af1aaff13e0.svg' alt='æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆï¼ˆä¸€å…ƒçº¿æ€§å›å½’ï¼‰'>

### è¯„ä¼°æ–¹æ³•

ç•™å‡ºæ³•ï¼ˆHold-outï¼‰ï¼šæ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

äº¤å‰éªŒè¯æ³•ï¼ˆCross Validationï¼‰ï¼šå°†æ•°æ®é›†åˆ†æˆ N å—ï¼Œä½¿ç”¨ N-1 å—è¿›è¡Œè®­ç»ƒï¼Œå†ç”¨æœ€åä¸€å—è¿›è¡Œæµ‹è¯•ï¼›

è‡ªåŠ©æ³•ï¼ˆBootstrapï¼‰ï¼š

ï¼ˆBaggingï¼‰ï¼š

### å›å½’æŒ‡æ ‡

#### MAE

MAEï¼ˆMean Absolute Errorï¼‰ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ã€‚

$$ MAE = \frac{1}{m} \sum_{i=1}^{m} \lvert \hat{y}^{(i)} - y^{(i)} \rvert $$

#### MAPE

MAPEï¼ˆMean Absolute Percentage Errorï¼‰ï¼Œå¹³å‡ç»å¯¹ç™¾åˆ†è¯¯å·®ã€‚

$$ MAPE = \frac{100}{m} \sum_{i=1}^{m} \lvert \frac{y^{(i)} - \hat{y}^{(i)}}{y^{(i)}} \rvert $$

#### MSE<a id="MSE"></a>

MSEï¼ˆMean Squared Errorï¼‰ï¼Œå‡æ–¹è¯¯å·®ã€‚æœ€å°äºŒä¹˜çš„å‡å€¼ç‰ˆï¼Œå¸¸ç”¨äºçº¿æ€§å›å½’æ¨¡å‹çš„æˆæœ¬å‡½æ•°ã€‚

$$ MSE = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 $$

#### RMSE

RMSEï¼ˆRoot Mean Square Errorï¼‰ï¼Œå‡æ–¹æ ¹è¯¯å·®ã€‚

$$ RMSE = \sqrt{MSE} $$

#### R<sup>2</sup><a id="Coefficient-of-Determination"></a>

R<sup>2</sup> (coefficient of determination)ï¼Œå†³å®šç³»æ•°ã€‚è¡¡é‡**æ€»è¯¯å·®ï¼ˆå®¢è§‚å­˜åœ¨ä¸”æ— å…³å›å½’æ¨¡å‹ï¼‰ä¸­å¯ä»¥è¢«å›å½’æ¨¡å‹è§£é‡Šçš„æ¯”ä¾‹**ï¼Œå³æ‹Ÿåˆç¨‹åº¦ã€‚

$$ R^2 = \frac{SSR}{SST} = 1- \frac{SSE}{SST} $$

è¯´æ˜ï¼š
å½“ $R^2 \to 1$ æ—¶ï¼Œè¡¨æ˜æ‹Ÿåˆç¨‹åº¦è¶Šå¥½ï¼Œå› ä¸ºæ­¤æ—¶ SSR è¶‹å‘äº SSTï¼ˆæˆ– SSE è¶‹å‘äº 0ï¼‰ï¼›
å½“ $R^2 \to 0$ æ—¶ï¼Œè¡¨æ˜æ‹Ÿåˆç¨‹åº¦è¶Šå·®ï¼Œå› ä¸ºæ­¤æ—¶ SSR è¶‹å‘äº 0ï¼ˆæˆ– SSE è¶‹å‘äº SSTï¼‰ï¼›

{{< expand "å…³äº SST/SSR/SSE">}}

{{< boxmd >}}
åŠ©è®°å°æŠ€å·§ï¼š**T** is short for total, **R** is short for regression, **E** is short for error.
{{< /boxmd >}}

SST (sum of squares total)ï¼Œæ€»å¹³æ–¹å’Œï¼Œç”¨äºè¡¡é‡**çœŸå®å€¼**ç›¸å¯¹**å‡å€¼**çš„ç¦»æ•£ç¨‹åº¦ã€‚SST å®¢è§‚å­˜åœ¨ä¸”ä¸å›å½’æ¨¡å‹æ— å…³ï¼›

$$ SST = \sum_{i=1}^{m} (y^{(i)} - \bar{y})^2 $$

SSR (sum of squares due to regression)ï¼Œå›å½’å¹³æ–¹å’Œï¼Œç”¨äºè¡¡é‡**é¢„æµ‹å€¼**ç›¸å¯¹**å‡å€¼**çš„ç¦»æ•£ç¨‹åº¦ã€‚å½“ SSR = SST æ—¶ï¼Œå›å½’æ¨¡å‹å®Œç¾ï¼›

$$ SSR = \sum_{i=1}^{m} (\hat{y}^{(i)} - \bar{y})^2 $$

SSE (sum of squares error)ï¼Œè¯¯å·®å¹³æ–¹å’Œï¼Œç”¨äºè¡¡é‡**é¢„æµ‹å€¼**ç›¸å¯¹**çœŸå®å€¼**çš„ç¦»æ•£ç¨‹åº¦ï¼›

$$ SSE = \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 $$

ä¸”ä¸‰è€…ä¹‹é—´çš„å…³ç³»æ˜¯ $SST = SSR + SSE$.

{{< /expand >}}

<img src='https://user-images.githubusercontent.com/46241961/273468625-e2263610-af8d-4ada-9cf9-9c25eef6c3c3.svg' alt='LinearRegression_SST_SSR_SSE' width='80%'>

### åˆ†ç±»æŒ‡æ ‡

#### æ··æ·†çŸ©é˜µ

ï¼ˆconfusion matrixï¼‰

ç”¨äºåˆ†ç±»æ¨¡å‹çš„æ•ˆæœè¯„ä¼°ã€‚ä»¥ä¸‹ä»¥äºŒåˆ†ç±»æ¨¡å‹ä¸ºä¾‹ï¼š

| é¢„æµ‹/å®é™…&nbsp;&nbsp;&nbsp; | Positive&nbsp;&nbsp;&nbsp; | Negative&nbsp;&nbsp;&nbsp; |
| ---------- | ---------- | ---------- |
| **Positive** | TP  | FP&nbsp;&nbsp;&nbsp; | 
| **Negative** | FN  | TN&nbsp;&nbsp;&nbsp; | 

- å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ï¼šæŒ‡é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ï¼Œå³ $\frac{TP+TN}{TP+TN+FP+FN}$
- ç²¾ç¡®ç‡ï¼ˆprecisionï¼‰ï¼šä¹Ÿç§°ä½œæŸ¥å‡†ç‡ï¼ŒæŒ‡é¢„æµ‹ä¸ºæ­£ä¸­å®é™…ä¸ºæ­£çš„æ¯”ä¾‹ï¼Œå³ $\frac{TP}{TP+FP}$
- å¬å›ç‡ï¼ˆrecallï¼‰ï¼šä¹Ÿç§°ä½œæŸ¥å…¨ç‡ï¼ŒæŒ‡å®é™…ä¸ºæ­£ä¸­é¢„æµ‹ä¸ºæ­£çš„æ¯”ä¾‹ï¼Œå³ $\frac{TP}{TP+FN}$
- F1ï¼š$\frac{2 \times	 ç²¾ç¡®ç‡ \times å¬å›ç‡}{ç²¾ç¡®ç‡ + å¬å›ç‡}$

#### ROC

[æ·±å…¥ä»‹ç´¹åŠæ¯”è¼ƒROCæ›²ç·šåŠPRæ›²ç·š](https://medium.com/nlp-tsupei/roc-pr-%E6%9B%B2%E7%B7%9A-f3faa2231b8c)

ç”¨äºåˆ†ç±»æ¨¡å‹çš„æ•ˆæœè¯„ä¼°ï¼Œä»¥å¯è§†åŒ–çš„æ–¹å¼ã€‚

## æ•°å­¦åŸºç¡€

### çŸ©

{{< alert theme="info" >}}
ä¸€é˜¶åŸç‚¹çŸ©æ˜¯[æœŸæœ›å€¼](#Expectation)ï¼ŒäºŒé˜¶ä¸­å¿ƒçŸ©æ˜¯[æ–¹å·®](#Variance)ï¼Œä¸‰é˜¶æ ‡å‡†çŸ©æ˜¯[ååº¦](#)ï¼Œå››é˜¶æ ‡å‡†çŸ©å‡å¸¸æ•°3æ˜¯[å³°åº¦](#)ï¼ŒäºŒé˜¶æ··åˆä¸­å¿ƒçŸ©æ˜¯[åæ–¹å·®](#Covariance)ã€‚
{{< /alert >}}

è®¾éšæœºå˜é‡ $X$ å’Œ $Y$ï¼Œæ­£æ•´æ•° $k$ å’Œ $l$.

#### åŸç‚¹çŸ©

**åŸç‚¹**æŒ‡`åæ ‡åŸç‚¹`ã€‚$X$ çš„ $k$ é˜¶åŸç‚¹çŸ©ï¼š

$$ E(X^k) $$

#### ä¸­å¿ƒçŸ©

**ä¸­å¿ƒ**æŒ‡`æœŸæœ›å€¼`ã€‚$X$ çš„ $k$ é˜¶ä¸­å¿ƒçŸ©è®°ä½œ $\mu_k$ï¼š

$$ \mu_k = E\lbrack X - E(X)\rbrack^k $$

#### æ ‡å‡†çŸ©

**æ ‡å‡†åŒ–**æŒ‡`é™¤ä»¥æ ‡å‡†å·®ä»¥å‰”é™¤é‡çº²`ï¼Œæ ‡å‡†çŸ©æ˜¯`æ ‡å‡†åŒ–çš„ä¸­å¿ƒçŸ©`ã€‚$X$ çš„ $k$ é˜¶æ ‡å‡†çŸ©ï¼š

$$
\frac{\mu_k}{\sigma^k} = \frac{\mu_k}{\mu_2^{\frac{k}{2}}} = \frac{E\lbrack X - E(X)\rbrack^k}{\left(E\lbrack X - E(X)\rbrack^2\right)^{\frac{k}{2}}}
$$

#### æ··åˆçŸ©

$X$ å’Œ $Y$ çš„ $k+l$ é˜¶æ··åˆçŸ©ï¼š

$$ E(X^kY^l) $$

#### æ··åˆä¸­å¿ƒçŸ©

$X$ å’Œ $Y$ çš„ $k+l$ é˜¶æ··åˆä¸­å¿ƒçŸ©ï¼š

$$ E\lbrace\lbrack X - E(X)\rbrack^k \lbrack Y - E(Y)\rbrack^l \rbrace $$

### ç»Ÿè®¡æŒ‡æ ‡

æ³¨æ„è¿™é‡Œä¸ä¸¥æ ¼åŒºåˆ†**æ€»ä½“**å’Œ**æ ·æœ¬**ï¼Œå¹¶ä½¿ç”¨æ ·æœ¬ä¼°è®¡æ•´ä½“ã€‚

#### æå·®

$$ \max(x) - \min(x) $$

#### æœŸæœ›å€¼<a id="Expectation"></a>

è¿™é‡Œä½¿ç”¨`æ ·æœ¬å‡å€¼`ä¼°è®¡`æ€»ä½“æœŸæœ›å€¼`ã€‚

$$ 
\begin{split}
\mu &= E(X) = \sum_{j=1}^{N} p(x_j) x_j \\\\ 
&\approx \bar{x} = \frac{1}{n} \sum_{j=1}^{n} x_j 
\end{split}
$$

{{< notice info>}}
è¯´æ˜ï¼šæ ¹æ®[å¼ºå¤§æ•°å®šå¾‹](#)ï¼Œå½“ n è¶‹å‘äºæ— ç©·æ—¶ï¼Œ`æ ·æœ¬å‡å€¼ä¾æ¦‚ç‡ 1 æ”¶æ•›äºæœŸæœ›å€¼`ã€‚
{{< /notice >}}

##### æœŸæœ›å€¼ä¸å‡å€¼

æ€»ä½“æœŸæœ›å€¼æ˜¯å¸¸æ•°æ ‡é‡ï¼Œæ ·æœ¬å‡å€¼ä¾èµ–äºå…·ä½“çš„éšæœºæŠ½æ ·ã€‚

å¦‚æŠ›ç¡¬å¸ï¼ˆ[ä¼¯åŠªåˆ©è¯•éªŒ](#BernoulliDistribution)ï¼‰ï¼Œæ€»ä½“æœŸæœ›å€¼æ˜¯ $0.5 * 1 + 0.5 * 0 = 0.5$ï¼Œä½†æ ·æœ¬å‡å€¼æ¯”å¦‚æŠ› 3 æ¬¡ $(1+1+0)/3 \neq 0.5$.

#### æ–¹å·®<a id="Variance"></a>

æ–¹å·®ï¼ˆVarianceï¼‰ç”¨äºè¡¡é‡ç›¸å¯¹å‡å€¼çš„`ç¦»æ•£ç¨‹åº¦`ã€‚

$$ 
\begin{split}
\sigma^2 &= Var(X) = E\lbrack X - E(X)\rbrack^2 = \sum_{j=1}^{N} p(x_j)(x_j - \mu)^2 \\\\
&\approx \frac{1}{n} \sum_{j=1}^{n} (x_j - \bar{x})^2 
\end{split}
$$

è¯´æ˜ï¼šè¶Šå¤§ï¼Œè¶Šæ‰ï¼Œè¶Šç¦»æ•£ï¼Œç†µè¶Šå¤§ã€‚

#### æ ‡å‡†å·®

æ ‡å‡†å·®ï¼ˆStandard deviationï¼‰æ˜¯æ–¹å·®çš„æ­£å¹³æ–¹æ ¹ã€‚

$$ \sigma = \sqrt{\sigma^2} $$

<!-- #### å˜å¼‚ç³»æ•°

å˜å¼‚ç³»æ•°ï¼ˆCoefficient of variationï¼ŒCVï¼‰ï¼Œæ˜¯æ ‡å‡†å·®çš„å½’ä¸€åŒ–ï¼Œæ— é‡çº²ã€‚

$$ c_v = \frac{\sigma}{\mu} $$ -->

#### åæ–¹å·®<a id="Covariance"></a>

åæ–¹å·®ï¼ˆCovarianceï¼‰ç”¨äºè¡¡é‡ä¸¤ä¸ªå˜é‡çš„`çº¿æ€§ç›¸å…³æ€§`ã€‚

$$
Cov(X,Y) = E\lbrace\lbrack X - E(X)\rbrack \lbrack Y - E(Y)\rbrack \rbrace
$$

è¯´æ˜ï¼š$Cov(X,X) = Var(X)$ï¼Œå³æ–¹å·®æ˜¯åæ–¹å·®çš„ç‰¹æ®Šæƒ…å½¢ã€‚

#### ç›¸å…³ç³»æ•°

æ ‡å‡†åŒ–çš„åæ–¹å·®ã€‚

$$
\rho = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}
$$

#### ååº¦<a id="Skewness"></a>

ååº¦ï¼ˆSkewnessï¼‰ç”¨äºè¡¡é‡åˆ†å¸ƒçš„`å¯¹ç§°æ€§`ã€‚

$$
Skewness = \frac{\mu_3}{\sigma^3} = \frac{\mu_3}{\mu_2^{\frac{3}{2}}}
$$

è¯´æ˜ï¼šå°¾å·´åœ¨å“ªè¾¹å°±åå“ªè¾¹ã€‚

<!-- <img src='' alt='è´Ÿåï¼ˆå·¦ï¼‰å’Œæ­£åï¼ˆå³ï¼‰ï¼ˆå›¾æºç»´åŸºç™¾ç§‘ï¼‰'> -->

#### å³°åº¦<a id="Kurtosis"></a>

å³°åº¦ï¼ˆKurtosisï¼‰ç”¨äºè¡¡é‡ç›¸å¯¹é«˜æ–¯åˆ†å¸ƒçš„`é™¡å³­ç¨‹åº¦`ã€‚

$$
Kurtosis = \frac{\mu_4}{\sigma^4} - 3 = \frac{\mu_4}{\mu_2^2} - 3
$$

è¯´æ˜ï¼šå‡å¸¸æ•° 3 æ˜¯ä¸ºäº†ä½¿é«˜æ–¯åˆ†å¸ƒçš„å³°åº¦ä¸ºé›¶ã€‚

### å¯¼æ•°

{{< alert theme="info" >}}
**ä¸€é˜¶å¯¼ç”¨äºå•è°ƒæ€§åˆ¤æ–­ï¼ŒäºŒé˜¶å¯¼ç”¨äºå‡¹å‡¸æ€§åˆ¤æ–­ã€‚**
{{< /alert >}}

ç»™å®šå‡½æ•° $f: \mathbb{R} \to \mathbb{R}$ï¼Œåˆ™ $f$ åœ¨ç‚¹ $x$ å¤„çš„ä¸€é˜¶å¯¼æ•° $f'$ å’ŒäºŒé˜¶å¯¼æ•° $f''$ çš„å®šä¹‰åˆ†åˆ«å¦‚ä¸‹ï¼š

$$
f' = \frac{dy}{dx} = \lim_{{\Delta x} \to 0} \frac{f(x + {\Delta x})}{\Delta x}
$$

$$ f'' = (f')' = \frac{d^2y}{dx^2} $$

æ³¨æ„ï¼šå¯å¯¼ç­‰äºå¯å¾®ï¼Œå¯å¯¼ä¸€å®šè¿ç»­ï¼›
è¯´æ˜ï¼šä¸€é˜¶å¯¼è¡¨ç¤ºå‡½æ•°åœ¨è¯¥ç‚¹å¤„çš„`ç¬æ—¶å˜åŒ–ç‡`ï¼›
ç”¨é€”ï¼šä¸€é˜¶å¯¼ç”¨äºåˆ¤æ–­**å•è°ƒæ€§**ï¼›äºŒé˜¶å¯¼ç”¨äºåˆ¤æ–­**å‡¹å‡¸æ€§**ï¼Œå¤§äºé›¶åˆ™å‡¸ï¼ˆU å‹ï¼‰ã€‚

### åå¯¼æ•°

ç»™å®šå‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ï¼Œåˆ™ $f$ å¯¹è‡ªå˜é‡ $x_j$ çš„åå¯¼æ•°ï¼ˆpartial derivativeï¼‰ï¼ŒæŒ‡å°†å…¶ä»–è‡ªå˜é‡è§†ä½œå¸¸é‡æ—¶ï¼Œå¯¹ $x_j$ çš„å¯¼æ•°ï¼Œå³ï¼š

$$ 
\frac{\partial f}{\partial x_j} = \lim_{{\Delta x_j} \to 0} \frac{f(x_j + {\Delta x_j}, ...) - f(x_j, ...)}{\Delta x_j}
$$

æ³¨æ„ï¼šå¯å¾®ä¸€å®šå¯å¯¼ï¼Œå¯å¾®ä¸€å®šè¿ç»­ã€‚

### æ¢¯åº¦

{{< alert theme="info" >}}
**æ¢¯åº¦æ˜¯ä¸€ä¸ªå‘é‡ï¼Œæ²¿ç€æ¢¯åº¦æ–¹å‘å‡½æ•°å€¼ä¸Šå‡æœ€å¿«ï¼Œé€†ç€æ¢¯åº¦æ–¹å‘å‡½æ•°å€¼ä¸‹é™æœ€å¿«ã€‚**
{{< /alert >}}

ç»™å®š`å¯å¾®`å‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ï¼Œåˆ™ $f$ çš„**åå¯¼æ•°æ„æˆçš„å‘é‡**ï¼Œç§°ä¸ºæ¢¯åº¦ï¼Œè®°ä½œ $grad f$ æˆ– $\nabla f$ï¼Œå³ï¼š

$$
grad f = \nabla f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},..., \frac{\partial f}{\partial x_n})
$$

ç”¨é€”ï¼š[æ¢¯åº¦ä¸‹é™ç®—æ³•](#GD)

### å‡¸å‡½æ•°

å¦‚æœä¸€ä¸ªå‡½æ•°æ»¡è¶³`ä»»æ„ä¸¤ç‚¹è¿æˆçš„çº¿æ®µéƒ½ä½äºå‡½æ•°å›¾å½¢çš„ä¸Šæ–¹`ï¼Œåˆ™ç§°è¿™ä¸ªå‡½æ•°ä¸ºå‡¸å‡½æ•°ï¼ˆConvex functionï¼‰ã€‚

å‡¸å‡½æ•°çš„å±€éƒ¨æœ€å°å€¼ç­‰äºæå°å€¼ï¼Œå¯ä½œä¸ºé€‰æ‹©æŸå¤±å‡½æ•°çš„é‡è¦å‚è€ƒã€‚

### å‘é‡

{{< alert theme="info" >}}
**ç‚¹ç§¯æ˜¯æ ‡é‡ï¼Œå‰ç§¯æ˜¯å‘é‡ï¼Œå¤–ç§¯æ˜¯çŸ©é˜µã€‚**
{{< /alert >}}

n ç»´å‘é‡ $x$ è®°ä½œï¼š

$$
x = \begin{bmatrix}x_1 \\\\ x_2 \\\\ \vdots \\\\ x_n \end{bmatrix} \in \mathbb{R}^n
$$

è¯´æ˜ï¼šæœ¬æ–‡ä¸€å¾‹é»˜è®¤åˆ—å‘é‡ï¼Œåœ¨ Python ä¸­å¯¹åº”ä¸€ç»´æ•°ç»„ã€‚$x$ ä¹Ÿå¯è§†ä½œä¸€ä¸ª $n \times 1$ çŸ©é˜µã€‚

#### ç‚¹ç§¯<a id="DotProduct"></a>

ç‚¹ç§¯ï¼ˆDot productï¼‰ï¼Œä¹Ÿç§°ä½œç‚¹ä¹˜ã€å†…ç§¯ã€æ•°é‡ç§¯ã€‚å¯¹äº $x,y \in \mathbb{R}^n$ï¼š

$$
x \cdot y = x^Ty = \sum_{j=1}^{n} x_jy_j \in \mathbb{R}
$$

æ³¨æ„ï¼šç›¸åŒç»´æ•°æ‰èƒ½è¿›è¡Œç‚¹ç§¯ä¹˜æ³•ï¼›
è¯´æ˜ï¼šå‡ ä½•æ„ä¹‰æ˜¯å‘é‡å›´æˆçš„å¹³é¢çš„`é¢ç§¯`æˆ–ç©ºé—´çš„`ä½“ç§¯`ï¼ˆæœ‰æ­£è´Ÿå·ï¼‰ï¼Œå¤§å°ç­‰äº $\lVert x \rVert \lVert y \rVert\cos(\theta)$ï¼Œå…¶ä¸­ $\theta$ ä¸ºä¸¤å‘é‡ä¹‹é—´çš„å¤¹è§’ï¼›
ç”¨é€”ï¼š[ä½™å¼¦ç›¸ä¼¼åº¦](#CosineSimilarity)

#### å‰ç§¯

å‰ç§¯ï¼ˆCross productï¼‰ï¼Œä¹Ÿç§°ä½œå‰ä¹˜ã€å‘é‡ç§¯ã€‚å¯¹äº $x,y \in \mathbb{R}^3$ï¼š

$$
\begin{split}
x \times y &= 
\left|
  \begin{matrix}
  \vec{i} & \vec{j} & \vec{k} \\\\ 
  x_1 & x_2 & x_3 \\\\
  y_1 & y_2 & y_3
  \end{matrix}
\right| \\\\
\\\\&= (x_2y_3-x_3y_2)\vec{i} - (x_1y_3-x_3y_1)\vec{j} + (x_1y_2-x_2y_1)\vec{k} \\\\
\\\\&= \begin{bmatrix}x_2y_3-x_3y_2 \\\\ -(x_1y_3-x_3y_1) \\\\ x_1y_2-x_2y_1 \end{bmatrix} \in \mathbb{R}^3
\end{split}
$$

æ³¨æ„ï¼šå‰ç§¯çš„æ¦‚å¿µä»…ç”¨äºä¸‰ç»´ç©ºé—´ã€‚è¿™é‡Œçš„å…¬å¼è¡¨è¾¾ä½¿ç”¨äº†[è¡Œåˆ—å¼](#Determinant)å’Œä»£æ•°ä½™å­å¼ï¼›
è¯´æ˜ï¼šå‡ ä½•æ„ä¹‰æ˜¯`æ³•å‘é‡`ï¼Œå¤§å°ç­‰äº $\lVert x \rVert \lVert y \rVert \sin(\theta)$ï¼Œå…¶ä¸­ $\theta$ ä¸ºä¸¤å‘é‡ä¹‹é—´çš„å¤¹è§’ã€‚

#### å¤–ç§¯

å¤–ç§¯ï¼ˆOuter productï¼‰ã€‚å¯¹äº $x \in \mathbb{R}^m, y \in \mathbb{R}^n$ï¼š

$$
x \otimes y = xy^T = 
\begin{bmatrix}
  x_1y_1 & x_1y_2 & \dots & x_1y_n \\\\ 
  x_2y_1 & x_2y_2 & \dots & x_2y_n \\\\ 
  \vdots & \vdots & \ddots & \vdots \\\\ 
  x_my_1 & x_my_2 & \dots & x_my_n
\end{bmatrix}
\in \mathbb{R}^{m \times n}
$$

è¯´æ˜ï¼šè¿ç®—ç»“æœæ˜¯ä¸ªçŸ©é˜µã€‚

### çŸ©é˜µ

$m \times n$ çŸ©é˜µå¯ç†è§£ä¸º n ä¸ªåˆ—å‘é‡çš„é›†åˆï¼ˆæˆ– m ä¸ªè¡Œå‘é‡çš„é›†åˆï¼‰ã€‚

#### çº¿æ€§ç›¸å…³

{{< alert theme="info" >}}
**n ä¸ªçº¿æ€§æ— å…³çš„å‘é‡ï¼Œå¯ä½œä¸ºåŸºå‘é‡ï¼Œå¼ æˆä¸€ä¸ª n ç»´ç©ºé—´ã€‚**
{{< /alert >}}

å¯¹äº n ä¸ªå‘é‡ $x_1,x_2,...,x_n$ï¼Œä»¤å…¶çº¿æ€§ç»„åˆä¸ºé›¶å‘é‡ï¼Œå³ç­‰å¼

$$ w_1x_1 + w_2x_2 + \cdots + w_nx_n = \vec{0},\space\space (n>=2) $$

å…¶ä¸­ $w_j \in \mathbb{R}$ã€‚**`å¦‚æœå½“ä¸”ä»…å½“ $w_1 = w_2 = \cdots = w_n = 0$ å³å…¨éƒ¨ç³»æ•°ä¸ºé›¶æ—¶æ‰æˆç«‹ï¼Œåˆ™ç§°è¯¥ n ä¸ªå‘é‡çº¿æ€§æ— å…³`**ï¼Œå¦åˆ™çº¿æ€§ç›¸å…³ã€‚

è¯´æ˜ï¼šçº¿æ€§ç›¸å…³ï¼Œåˆ™å…¶ä¸­ä¸€ä¸ªå¯ä»¥ç”¨å…¶ä½™çš„çº¿æ€§ç»„åˆè¡¨ç¤ºï¼Œæ­¤æ—¶å¯é™ç»´ã€‚

{{< notice info >}}
çº¿æ€§æ— å…³ï¼Œå¯¹äº n å– 2 å°±æ˜¯ä¸¤ä¸ªå‘é‡ä¸å…±çº¿ï¼Œå¯¹äº n å– 3 å°±æ˜¯ä¸‰ä¸ªå‘é‡ä¸å…±é¢ã€‚
{{< /notice >}}

#### ç§©

{{< alert theme="info" >}}
çŸ©é˜µçš„ç§©ç­‰äºçº¿æ€§æ— å…³çš„åˆ—å‘é‡çš„ä¸ªæ•°ã€‚æ»¡ç§©åˆ™çº¿æ€§æ— å…³ï¼Œä¸æ»¡ç§©åˆ™çº¿æ€§ç›¸å…³ã€‚
{{< /alert >}}

çŸ©é˜µçš„ç§©ï¼ˆRankï¼‰è®°ä½œ $rank$ï¼Œä¸”`ç§© = åˆ—ç§© = è¡Œç§©`ã€‚

å¯¹äº $X \in \mathbb{R}^{m \times n}$ï¼Œç”±äºå®é™…ä¸­ $m \gg n$ï¼Œå› æ­¤å…¶ç§©ç”± $n$ å†³å®šã€‚ä¸”ï¼š
- è‹¥ $rank(X) = n$ï¼Œå³åˆ—æ»¡ç§©ï¼Œåˆ™ n ä¸ªç‰¹å¾çº¿æ€§æ— å…³ï¼›
- è‹¥ $rank(X) < n$ï¼Œå³åˆ—ä¸æ»¡ç§©ï¼Œåˆ™ n ä¸ªç‰¹å¾çº¿æ€§ç›¸å…³ï¼Œæ­¤æ—¶`é™ç»´`èµ°èµ·ã€‚

#### è¡Œåˆ—å¼<a id="Determinant"></a>

è¡Œåˆ—å¼ï¼ˆDeterminantï¼‰é’ˆå¯¹çš„æ˜¯ `n é˜¶æ–¹é˜µ`ï¼Œè®°ä½œ $\det$.


#### çŸ©é˜µä¹˜å‘é‡

{{< alert theme="info" >}}
**çŸ©é˜µæ˜¯ä¸€ç»„çº¿æ€§å˜æ¢çš„ç»„åˆ**ã€‚
{{< /alert >}}

ç†è§£ï¼šå°†çŸ©é˜µçš„åˆ—å‘é‡çœ‹ä½œä¸€ç»„æ–°çš„**ä¼ªåŸºå‘é‡**ï¼Œåˆ™çŸ©é˜µä¹˜å‘é‡å¯ä»¥ç†è§£ä¸º**å¯¹å‘é‡è¿›è¡Œä¸€æ¬¡çº¿æ€§å˜æ¢**ã€‚

$$
\begin{bmatrix}a & b & c \\\\ d & e & f \\\\ g & h & i \end{bmatrix}
\begin{bmatrix}x \\\\ y \\\\ z \end{bmatrix} = 
x \begin{bmatrix}a \\\\ d \\\\ g \end{bmatrix} + 
y \begin{bmatrix}b \\\\ e \\\\ h \end{bmatrix} + 
z \begin{bmatrix}c \\\\ f \\\\ i \end{bmatrix} = 
\begin{bmatrix}ax+by+cz \\\\ dx+ey+fz \\\\ gx+hy+iz \end{bmatrix}
$$

ç‰¹åˆ«çš„ï¼Œå½“çŸ©é˜µå–å•ä½çŸ©é˜µæ—¶ï¼Œè¯¥å‘é‡ç»è¿‡çº¿æ€§å˜åŒ–åï¼Œä»ç­‰äºè¯¥å‘é‡ã€‚

$$
\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \end{bmatrix}
\begin{bmatrix}x \\\\ y \\\\ z \end{bmatrix} = 
x \begin{bmatrix}1 \\\\ 0 \\\\ 0 \end{bmatrix} + 
y \begin{bmatrix}0 \\\\ 1 \\\\ 0 \end{bmatrix} + 
z \begin{bmatrix}0 \\\\ 0 \\\\ 1 \end{bmatrix} = 
\begin{bmatrix}x \\\\ y \\\\ z \end{bmatrix}
$$

#### çŸ©é˜µä¹˜çŸ©é˜µ

ç†è§£ï¼šå¤šæ¬¡çº¿æ€§å˜åŒ–çš„å åŠ ã€‚

### èŒƒæ•°

{{< alert theme="info" >}}
**èŒƒæ•°æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºé‡åŒ–å‘é‡æˆ–çŸ©é˜µçš„å¤§å°ï¼Œå³å°†å‘é‡æˆ–çŸ©é˜µæ˜ å°„ä¸ºä¸€ä¸ªæ ‡é‡ã€‚**
{{< /alert >}}

#### å‘é‡èŒƒæ•°<a id="VectorNorms"></a>

n ç»´å‘é‡ $x$ çš„ p èŒƒæ•°å®šä¹‰å¦‚ä¸‹ï¼š

$$ 
L_p(x) = \lVert x \rVert_p = \left(\sum_{j=1}^{n} {\lvert x_j \rvert}^p\right)^{1/p}
$$

åˆ™å½“ p ä¾æ¬¡å– $-\infty, 1, 2, +\infty$ æ—¶ï¼Œåˆ†åˆ«å¯¹åº”å¦‚ä¸‹èŒƒæ•°ï¼š

$$ 
\lVert x \rVert_{-\infty} = \lim_{p \to -\infty} \left(\sum_{j=1}^{n} {\lvert x_j \rvert}^p\right)^{1/p} = 
\min_{j} {\lvert x_j \rvert} \tag{$L_{-\infty}$}
$$

$$ 
\lVert x \rVert_1 = \sum_{j=1}^{n} {\lvert x_j \rvert} \tag{$L_1$}
$$

$$ 
\lVert x \rVert_2 = \left(\sum_{j=1}^{n} {\lvert x_j \rvert}^2\right)^{1/2} \tag{$L_2$}
$$

$$ 
\lVert x \rVert_{+\infty} = \lim_{p \to +\infty} \left(\sum_{j=1}^{n} {\lvert x_j \rvert}^p\right)^{1/p} = 
\max_{j} {\lvert x_j \rvert} \tag{$L_{+\infty}$}
$$

è¡¥å……è¯´æ˜ï¼š
1. L1 èŒƒæ•°ï¼Œä¹Ÿç§°ä½œ[æ›¼å“ˆé¡¿è·ç¦»](#ManhattanDistance)ï¼›
2. L2 èŒƒæ•°ï¼Œä¹Ÿç§°ä½œ[æ¬§æ°è·ç¦»](#EuclideanDistance)ï¼Œå¯ç”¨äºè®¡ç®—å‘é‡çš„æ¨¡ï¼ˆæœ¬æ–‡é»˜è®¤çœç•¥ä¸‹æ ‡ 2ï¼‰ï¼›
3. L$+\infty$ èŒƒæ•°ï¼Œä¹Ÿç§°ä½œ[åˆ‡æ¯”é›ªå¤«è·ç¦»](#ChebyshevDistance)æˆ–æœ€å¤§èŒƒæ•°ï¼›

#### çŸ©é˜µèŒƒæ•°<a id="MatrixNorms"></a>

### è·ç¦»å’Œç›¸ä¼¼åº¦

{{< alert theme="info" >}}
è·ç¦»å’Œç›¸ä¼¼åº¦å¸¸ç”¨äºåˆ†/èšç±»ï¼Œè·ç¦»è¶Šè¿‘æˆ–ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œåˆ™è¢«è®¤ä¸ºå¯ä»¥åˆ†/èšä¸ºä¸€ç±»ã€‚
{{< /alert >}}

å¯¹äºå‘é‡ $x,y \in \mathbb{R}^n$ï¼Œæˆ–ç©ºé—´ä¸­ä¸¤ä¸ªç‚¹ï¼Œè®¡ç®—è·ç¦»å¯ä½¿ç”¨`å·®å‘é‡çš„å¤§å°çš„è¡¡é‡`å¦‚èŒƒæ•°ï¼Œè®¡ç®—ç›¸ä¼¼åº¦å¯é€šè¿‡`ä¸¤å‘é‡å¤¹è§’`ç­‰æ¥è¡¡é‡ã€‚

#### é—µå¯å¤«æ–¯åŸºè·ç¦»<a id="MinkowskiDistance"></a>

æ˜¯å«å‚æ•° p çš„è·ç¦»å‡½æ•°ã€‚å½“ p ä¾æ¬¡å– 1, 2, $\infty$ æ—¶ï¼Œåˆ†åˆ«å¯¹åº”æ›¼å“ˆé¡¿è·ç¦»ã€æ¬§æ°è·ç¦»ã€åˆ‡æ¯”é›ªå¤«è·ç¦»ï¼›

$$ \left(\sum_{j=1}^{n} {\lvert x_j - y_j \rvert}^p\right)^{1/p} \tag{$L_p$} $$

#### æ›¼å“ˆé¡¿è·ç¦»<a id="ManhattanDistance"></a>

$$ \sum_{j=1}^{n} \lvert x_j - y_j \rvert \tag{$L_1$} $$

#### æ¬§æ°è·ç¦»<a id="EuclideanDistance"></a>

$$ \sqrt{\sum_{j=1}^{n} (x_j - y_j)^2} \tag{$L_2$} $$

#### åˆ‡æ¯”é›ªå¤«è·ç¦»<a id="ChebyshevDistance"></a>

$$ \max_{j} {\lvert x_j - y_j \rvert} \tag{$L_{+\infty}$} $$

<!-- #### æµ·æ˜è·ç¦» -->

<!-- #### é©¬æ°è·ç¦»

ï¼Ÿï¼Ÿåæ–¹å·®è·ç¦» -->

<!-- #### æ°å¡å¾·è·ç¦» -->

#### ä½™å¼¦ç›¸ä¼¼åº¦<a id="CosineSimilarity"></a>

ä½¿ç”¨`ä¸¤ä¸ªå‘é‡å¤¹è§’çš„ä½™å¼¦å€¼`æ¥è¡¡é‡ç›¸ä¼¼åº¦ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

$$ Cosine \space Similarity = \cos(\theta) = \frac{x \cdot y}{\lVert x \rVert \lVert y \rVert} $$

è¯´æ˜ï¼šç”±[å‘é‡ç‚¹ç§¯](#DotProduct)è®¡ç®—å…¬å¼æ¨å¯¼è€Œæ¥ã€‚è¶Šæ¥è¿‘äº 1ï¼Œå¤¹è§’è¶Šæ¥è¿‘äº 0ï¼Œè¶Šç›¸ä¼¼ã€‚

#### çš®å°”é€Šç›¸å…³ç³»æ•°

ä½¿ç”¨`æ ‡å‡†åŒ–åçš„åæ–¹å·®`æ¥è¡¡é‡ä¸¤ä¸ªéšæœºå˜é‡çš„`çº¿æ€§ç›¸å…³æ€§`ã€‚

$$
\rho = \frac{Cov(X,Y)}{\sigma_X \sigma_Y} \in [-1, 1]
$$

è¯´æ˜ï¼šè¶Šæ¥è¿‘äº 1 è¶Šæ­£çº¿æ€§ç›¸å…³ï¼Œè¶Šæ¥è¿‘äº -1 è¶Šè´Ÿçº¿æ€§ç›¸å…³ï¼Œç­‰äº 0 ä¸çº¿æ€§ç›¸å…³ã€‚

#### KL æ•£åº¦<a id="KLDivergence"></a>

ç»™å®š`ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ` $p(x)$ å’Œ $q(x)$ï¼Œä½¿ç”¨ KL æ•£åº¦æ¥è¡¡é‡ä¸¤è€…ä¹‹é—´çš„å·®å¼‚ç¨‹åº¦ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

$$ D_{KL}(p||q) = \sum_x p(x) \ln \frac{p(x)}{q(x)} \in [0, \infty] $$

è¯´æ˜ï¼šä¹Ÿç§°ä½œ[ç›¸å¯¹ç†µ](#KLD)ã€‚éè´Ÿï¼Œè¶Šå°è¶Šç›¸ä¼¼ã€‚

### è´å¶æ–¯å®šç†

è´å¶æ–¯å®šç†ï¼ˆBayes'theoremï¼‰å…¬å¼å¦‚ä¸‹ï¼ˆå…¶ä¸­ $P(B) \neq 0$ï¼‰ï¼š

$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$

è¯´æ˜ï¼š
1. å¯ç”±æ¡ä»¶æ¦‚ç‡ $P(A,B) = P(A|B)P(B) = P(B|A)P(A)$ æ¨å¯¼å¾—åˆ°ï¼›
2. $P(A|B)$ æ˜¯ $A$ çš„åéªŒæ¦‚ç‡ï¼Œ$P(A)$ æ˜¯ $A$ çš„å…ˆéªŒæ¦‚ç‡ï¼Œ$\frac{P(B|A)}{P(B)}$ ç§°ä½œ**æ ‡å‡†ä¼¼ç„¶åº¦**ï¼Œå› æ­¤è´å¶æ–¯å…¬å¼å¯è¡¨ç¤ºä¸ºï¼š$$ A çš„åéªŒæ¦‚ç‡ = A çš„å…ˆéªŒæ¦‚ç‡ * æ ‡å‡†ä¼¼ç„¶åº¦ $$

åŸºç¡€çŸ¥è¯†èƒŒæ™¯è§ä¸‹æ–¹ã€‚

#### è”åˆæ¦‚ç‡

$A$ å’Œ $B$ åŒæ—¶å‘ç”Ÿçš„æ¦‚ç‡ï¼Œè®°ä½œ $P(A,B)$ æˆ– $P(AB)$ æˆ– $P(A \cap B)$.

#### æ¡ä»¶æ¦‚ç‡

$B$ å‘ç”Ÿçš„æ¡ä»¶ä¸‹ $A$ å‘ç”Ÿçš„æ¦‚ç‡ï¼Œè®°ä½œ $A$ çš„æ¡ä»¶æ¦‚ç‡ $P(A|B)$ï¼Œå…¶ä¸­ $P(B) \neq 0$ï¼š$$ P(A|B) = \frac{P(A,B)}{P(B)} $$

#### å…ˆéªŒæ¦‚ç‡

`ä»¥ç»éªŒè¿›è¡Œåˆ¤æ–­`ï¼Œå¦‚ $P(A)$.

#### åéªŒæ¦‚ç‡

`ä»¥ç»“æœè¿›è¡Œåˆ¤æ–­`ã€‚å½“æ¡ä»¶æ¦‚ç‡ $P(A|B)$ ä¸­éšå« $A$ï¼ˆ`å› `ï¼‰ä¼šå¯¼è‡´ $B$ï¼ˆ`æœ`ï¼‰å‘ç”Ÿæ—¶ï¼Œåˆ™ç§°æ­¤æ¡ä»¶æ¦‚ç‡ä¸º $A$ çš„åéªŒæ¦‚ç‡ï¼Œå¯ç†è§£ä¸º **$P(å› |æœ)$**ã€‚

#### ç›¸äº’ç‹¬ç«‹

$A$ ä¸ $B$ ç›¸äº’ç‹¬ç«‹ï¼Œå½“ä¸”ä»…å½“ä»¥ä¸‹æˆç«‹ï¼š

$$P(A,B) = P(A)P(B)$$ 

{{< notice info>}}
æœ´ç´ è´å¶æ–¯**æœ´ç´ **åœ¨å‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚
{{< /notice >}}

### æ¦‚ç‡åˆ†å¸ƒå‡½æ•°

ç¦»æ•£å‹éšæœºå˜é‡å¯¹åº”`æ¦‚ç‡è´¨é‡å‡½æ•°`ï¼ˆProbability Mass Function, PMFï¼‰ï¼Œè¿ç»­å‹éšæœºå˜é‡å¯¹åº”`æ¦‚ç‡å¯†åº¦å‡½æ•°`ï¼ˆProbability Density Function, PDFï¼‰ã€‚

#### å‡åŒ€åˆ†å¸ƒ<a id="å‡åŒ€åˆ†å¸ƒ"></a>

`ç¦»æ•£å‹`éšæœºå˜é‡ $X = \lbrace x_1,x_2,\cdots,x_n \rbrace$ æœä»å‡åŒ€åˆ†å¸ƒï¼Œåˆ™ï¼š

$$ p(X=x) = \frac{1}{n} \tag{PMF} $$

`è¿ç»­å‹`éšæœºå˜é‡ $X \in [a,b]$ æœä»å‡åŒ€åˆ†å¸ƒï¼Œåˆ™ï¼š

$$
p(X=x) =
\begin{cases}
\frac{1}{b-a} & \text{if $x \in [a,b]$} \\\\
\\\\0 & \text{if $x \notin [a,b]$}
\end{cases} \tag{PMF}
$$

#### ä¼¯åŠªåˆ©åˆ†å¸ƒ<a id="BernoulliDistribution"></a>

`ä¼¯åŠªåˆ©è¯•éªŒ`æŒ‡æ¯æ¬¡è¯•éªŒçš„ç»“æœåªæœ‰ä¸¤ç§å¯èƒ½ï¼Œå¦‚æœæˆåŠŸï¼ˆ1ï¼‰çš„æ¦‚ç‡æ˜¯ $\phi$ï¼Œåˆ™å¤±è´¥ï¼ˆ0ï¼‰çš„æ¦‚ç‡æ˜¯ $1-\phi$.

ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆBernoulli distributionï¼‰ï¼Œä¹Ÿç§°ä½œ 0-1 åˆ†å¸ƒï¼ŒæŒ‡`ä¸€æ¬¡ä¼¯åŠªåˆ©è¯•éªŒ`ä¸­ï¼ŒæˆåŠŸï¼ˆ1ï¼‰çš„æ¬¡æ•°çš„æ¦‚ç‡åˆ†å¸ƒã€‚`ç¦»æ•£å‹`éšæœºå˜é‡ $X$ æœä»å‚æ•° $\phi$ çš„ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œè®°ä½œï¼š

$$
X \sim Bernoulli(\phi)
$$

å…¶æ¦‚ç‡è´¨é‡å‡½æ•°ã€æœŸæœ›å€¼å’Œæ–¹å·®åˆ†åˆ«å¦‚ä¸‹ï¼š

$$
\begin{split}
p(X=x;\phi) &= 
\begin{cases}
\phi & \text{if $x=1$} \\\\ \\\\
1-\phi & \text{if $x=0$} 
\end{cases} \\\\ 
\\\\&= \phi^x(1-\phi)^{1-x} 
\end{split} \tag{PMF}
$$

$$ \mu = \sum_{x} p(x) x = \phi $$

$$ \sigma^2 = \sum_{x} p(x) \left(x - \mu\right)^2 = \phi(1-\phi) $$

#### äºŒé¡¹åˆ†å¸ƒ

äºŒé¡¹åˆ†å¸ƒï¼ˆBinomial distributionï¼‰æŒ‡`é‡å¤ n æ¬¡ä¼¯åŠªåˆ©è¯•éªŒ`ï¼ŒæˆåŠŸï¼ˆ1ï¼‰çš„æ¬¡æ•°çš„æ¦‚ç‡åˆ†å¸ƒã€‚`ç¦»æ•£å‹`éšæœºå˜é‡ $X$ æœä»å‚æ•° $n, \phi$ çš„äºŒé¡¹åˆ†å¸ƒï¼Œè®°ä½œï¼š

$$
X \sim B(n, \phi)
$$

å…¶æ¦‚ç‡è´¨é‡å‡½æ•°ã€æœŸæœ›å€¼å’Œæ–¹å·®åˆ†åˆ«å¦‚ä¸‹ï¼Œå…¶ä¸­ $x \in \lbrace 0, 1, ..., n \rbrace$ï¼š

$$
p(X=x;n,\phi) = \frac{n!}{x!(n-x)!} \phi^x (1-\phi)^{n-x}  \tag{PMF}
$$

$$ \mu = n\phi $$

$$ \sigma^2 = n\phi(1-\phi)$$

<img src='https://user-images.githubusercontent.com/46241961/278027246-01e7fc5c-66b1-4b79-b855-002f64756da9.svg' alt='äºŒé¡¹åˆ†å¸ƒï¼šï¼ˆ10, 0.5ï¼‰' width=70%>

<br>è¯´æ˜ï¼šä¼¯åŠªåˆ©åˆ†å¸ƒæ˜¯ n å– 1 æ—¶çš„äºŒé¡¹åˆ†å¸ƒã€‚

#### é«˜æ–¯åˆ†å¸ƒ<a id="GaussianDistribution"></a>

é«˜æ–¯åˆ†å¸ƒï¼ˆGaussian distributionï¼‰ï¼Œä¹Ÿç§°ä½œæ­£æ€åˆ†å¸ƒï¼ˆNormal distributionï¼‰ã€‚`è¿ç»­å‹`éšæœºå˜é‡ $X$ æœä»å‡å€¼ $\mu$ï¼Œæ–¹å·® $\sigma^2$ çš„æ­£æ€åˆ†å¸ƒï¼Œè®°ä½œï¼š

$$
X \sim N(\mu, \sigma^2)
$$

å…¶æ¦‚ç‡å¯†åº¦å‡½æ•°å¦‚ä¸‹ï¼š

$$
p(X=x;\mu,\sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(-\frac{(x-\mu)^2} {2 \sigma^2}\right)  \tag{PDF}
$$

<img src='https://user-images.githubusercontent.com/46241961/278098372-7c4fe92c-e433-4c38-b7a0-06964ff05b12.svg' alt='é«˜æ–¯åˆ†å¸ƒ' width=70%>

<br>è¯´æ˜ï¼š**æ–¹å·®è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šåˆ†æ•£ï¼ˆæ··ä¹±ï¼‰ï¼Œè¶Šæ‰ï¼Œç†µè¶Šå¤§ï¼ˆå¹³å‡ä¿¡æ¯é‡è¶Šå¤§ï¼‰ã€‚**
<!-- 
{{< notice info>}}
å½“äºŒé¡¹åˆ†å¸ƒçš„ $\phi$ å– 0.5ï¼ˆå¯¹ç§°ï¼‰ï¼Œn è¶‹äºæ— ç©·ï¼ˆè¿ç»­ï¼‰æ—¶ï¼Œè¿‘ä¼¼é«˜æ–¯åˆ†å¸ƒã€‚
{{< /notice >}} -->

<!-- #### æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ -->


<!-- #### æŒ‡æ•°åˆ†å¸ƒ -->


<!-- #### æ³Šæ¾åˆ†å¸ƒ -->

### ç†µ<a id="Entropy"></a>

{{< alert theme="info" >}}
**ä¿¡æ¯é‡**æ˜¯ä¿¡æ¯çš„å¤§å°ï¼Œ**ç†µ**æ˜¯ä¿¡æ¯é‡çš„æœŸæœ›å€¼ï¼Œ**ç›¸å¯¹ç†µ**ç”¨äºè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œ**äº¤å‰ç†µ**æ˜¯ç›¸å¯¹ç†µçš„ç®€åŒ–ç‰ˆã€‚
{{< /alert >}}

#### ä¿¡æ¯é‡

ç»™å®šéšæœºå˜é‡ $x$ çš„æ¦‚ç‡åˆ†å¸ƒ $p(x) \in [0,1]$ï¼Œåˆ™ $x$ çš„`ä¿¡æ¯é‡`å®šä¹‰å¦‚ä¸‹ï¼š

$$ I(x) = \ln \frac{1}{p(x)} = - \ln p(x) $$

å…¶ä¸­ $\displaystyle \sum_x p(x) = 1$.

<img src='https://user-images.githubusercontent.com/46241961/278089095-219f103a-45e8-4920-825a-ef5f72e1832c.svg' alt='ä¿¡æ¯é‡' width=70%>

<br>è¯´æ˜ï¼š
1. æ¦‚ç‡è¶Šå°ï¼Œä¿¡æ¯é‡è¶Šå¤§ï¼›
2. å¯¹æ•°åº•æ•°ä»…å½±å“é‡åŒ–çš„å•ä½ï¼Œä»¥ 2 ä¸ºåº•å¯¹åº”æ¯”ç‰¹ï¼Œä»¥ e ä¸ºåº•å¯¹åº”çº³ç‰¹ï¼ˆé»˜è®¤ï¼‰ã€‚

#### ç†µ

ç†µï¼ˆEntropyï¼‰ç­‰äº $x$ çš„ **`ä¿¡æ¯é‡çš„æœŸæœ›å€¼`**ï¼Œç”¨äºè¡¡é‡**æ··ä¹±ç¨‹åº¦æˆ–ä¸ç¡®å®šæ€§**ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š

$$ 
H(p) = E(I(x)) = \sum_x p(x) I(x) = - \sum_x p(x) \ln p(x)
$$

<img src='https://user-images.githubusercontent.com/46241961/278096679-4b948c28-8618-43c6-85c0-3d52de6b4c61.svg' alt='ä¸åŒé«˜æ–¯åˆ†å¸ƒçš„ç†µå¯¹æ¯”' width=70%>

<br>è¯´æ˜ï¼š
1. **æ–¹å·®è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šåˆ†æ•£ï¼ˆæ··ä¹±ï¼‰ï¼Œç†µè¶Šå¤§ï¼ˆå¹³å‡ä¿¡æ¯é‡è¶Šå¤§ï¼‰ã€‚** [ï¼ˆğŸ‘ˆ æ¢…å¼€äºŒåº¦ï¼‰](#GaussianDistribution)
2. ç¦»æ•£å‹éšæœºå˜é‡å¯¹åº”**æ±‚å’Œ**ï¼Œè¿ç»­å‹éšæœºå˜é‡å¯¹åº”**æ±‚ç§¯åˆ†**ï¼ˆå·²çœç•¥ï¼‰ï¼›

#### ç›¸å¯¹ç†µ<a id="KLD"></a>

ç›¸å¯¹ç†µï¼ˆRelative Entropyï¼‰ï¼Œåˆç§°ä¸º `KL æ•£åº¦`ï¼ˆKullback-Leibler divergenceï¼‰ï¼Œç”¨äº`è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ç¨‹åº¦`ã€‚å¯¹äºä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ $p(x)$ å’Œ $q(x)$ï¼Œå…¶ç›¸å¯¹ç†µå®šä¹‰å¦‚ä¸‹ï¼š

$$ D_{KL}(p||q) = \sum_x p(x) \ln \frac{p(x)}{q(x)} $$

è¯´æ˜ï¼šéè´Ÿï¼Œä¸”è¶Šå°ï¼Œåˆ™ $p(x)$ å’Œ $q(x)$ åˆ†å¸ƒè¶Šæ¥è¿‘ï¼›

{{< expand "è¯æ˜ï¼šç›¸å¯¹ç†µéè´Ÿ" >}}
ç”±äº $\ln(x) \leq x - 1$ï¼Œåˆ™ï¼š

$$
\begin{split}
\- D_{KL}(p||q) &= \sum_x p(x) \ln \frac{q(x)}{p(x)} \\\\ 
&\leq \sum_x p(x) (\frac{q(x)}{p(x)} - 1) &= \sum_x (q(x) - p(x)) = 0
\end{split}
$$

å› æ­¤ $D_{KL}(p||q) \geq 0$ï¼Œå½“ä¸”ä»…å½“ $p(x) = q(x)$ æ—¶ä¸ºé›¶ã€‚
{{< /expand >}}

#### äº¤å‰ç†µ<a id="CrossEntropy"></a>

å°†ä¸Šè¿°ç›¸å¯¹ç†µå…¬å¼å±•å¼€ï¼š

$$ 
\begin{split}
D_{KL}(p||q) &= \sum_x p(x) \ln \frac{p(x)}{q(x)} \\\\
&= \sum_x p(x) \ln p(x) - \sum_x p(x) \ln q(x) \\\\
&= -H(p) + H(p,q)
\end{split}
$$

å…¶ä¸­ï¼Œå‰åŠéƒ¨åˆ†å°±æ˜¯`è´Ÿçš„ $p(x)$ çš„ç†µ`ï¼ŒååŠéƒ¨åˆ†åˆ™å°±æ˜¯`äº¤å‰ç†µ`ï¼ˆCross Entropyï¼‰ï¼š$$ H(p,q) = - \sum_x p(x) \ln q(x) $$

å®é™…åº”ç”¨ä¸­ï¼Œå¦‚æœå°† $p(x)$ ä½œä¸ºçœŸå®å€¼çš„æ¦‚ç‡åˆ†å¸ƒï¼Œ$q(x)$ ä½œä¸ºé¢„æµ‹å€¼çš„æ¦‚ç‡åˆ†å¸ƒï¼Œåˆ™ç”±äºçœŸå®å€¼çš„ç†µ $H(p)$ æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå› æ­¤ï¼š

$$ D_{KL}(p||q) \simeq H(p,q)$$

## é™„

ä¸€äº›æœ¯è¯­æ¦‚å¿µï¼š
- ç‰¹å¾å‘é‡ï¼šçŸ©é˜µçš„ç‰¹å¾å‘é‡ã€‚æ•°æ®é›†ç»“æ„çš„éé›¶å‘é‡ï¼›ç©ºé—´ä¸­æ¯ä¸ªç‚¹å¯¹åº”çš„ä¸€ä¸ªåæ ‡å‘é‡ã€‚

<!-- <img src='https://www.nvidia.cn/content/dam/en-zz/Solutions/gtcf20/data-analytics/nvidia-ai-data-science-workflow-diagram.svg'>

<img src='https://easyai.tech/wp-content/uploads/2022/08/523c0-2019-08-21-application.png.webp'>

<img src='https://www.tibco.com/sites/tibco/files/media_entity/2021-05/random-forest-diagram.svg'>

<img src='https://miro.medium.com/v2/resize:fit:1204/format:webp/1*iWHiPjPv0yj3RKaw0pJ7hA.png'> -->